
import time
import logging
from typing import List, Dict, Any
from FlagEmbedding import FlagReranker

logging.basicConfig(level=logging.ERROR) # Less logs
logger = logging.getLogger(__name__)

MODELS = [
    "BAAI/bge-reranker-v2-m3",
    "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1", # Corrected Name
    "jinaai/jina-reranker-v2-base-multilingual"
]

TEST_DATA = [
    {
        "query": "Thá»i gian lÃ m viá»‡c tá»‘i Ä‘a lÃ  bao nhiÃªu giá»?",
        "docs": [
            "åŠ´åƒæ™‚é–“ã¯ã€ä¼‘æ†©æ™‚é–“ã‚’é™¤ãã€1æ—¥ã«ã¤ã„ã¦8æ™‚é–“ã‚’è¶…ãˆã¦åŠ´åƒã•ã›ã¦ã¯ãªã‚‰ãªã„ã€‚",
            "ä½¿ç”¨è€…ã¯ã€åŠ´åƒè€…ã«å¯¾ã—ã€æ¯é€±å°‘ãã¨ã‚‚1å›ã®ä¼‘æ—¥ã‚’ä¸ãˆãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚",
            "NgÆ°á»i sá»­ dá»¥ng lao Ä‘á»™ng khÃ´ng Ä‘Æ°á»£c cho ngÆ°á»i lao Ä‘á»™ng lÃ m viá»‡c quÃ¡ 8 giá» má»™t ngÃ y.",
            "Má»—i tuáº§n ngÆ°á»i lao Ä‘á»™ng Ä‘Æ°á»£c nghá»‰ Ã­t nháº¥t 24 giá» liÃªn tá»¥c."
        ]
    },
    {
        "query": "Thá»§ tá»¥c xin nghá»‰ viá»‡c nhÆ° tháº¿ nÃ o?",
        "docs": [
            "ÄÆ°Æ¡ng sá»± muá»‘n xin thÃ´i viá»‡c pháº£i lÃ m Ä‘Æ¡n trÆ°á»›c Ã­t nháº¥t 30 ngÃ y.",
            "Tiá»n lÆ°Æ¡ng Ä‘Æ°á»£c tráº£ báº±ng tiá»n máº·t hoáº·c chuyá»ƒn khoáº£n.",
            "é€€è·ã—ã‚ˆã†ã¨ã™ã‚‹è€…ã¯ã€å°‘ãªãã¨ã‚‚30æ—¥å‰ã«ç”³ã—å‡ºãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚",
            "è³ƒé‡‘ã¯ã€é€šè²¨ã§ã€ç›´æ¥åŠ´åƒè€…ã«ã€ãã®å…¨é¡ã‚’æ”¯æ‰•ã‚ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚"
        ]
    }
]

def benchmark_model(model_name: str):
    print(f"\n{'='*50}")
    print(f"ğŸš€ Benchmarking: {model_name}")
    
    # 1. Load Time
    start_load = time.time()
    try:
        # trust_remote_code=True for Jina
        reranker = FlagReranker(model_name, use_fp16=False, device='cpu', trust_remote_code=True)
    except Exception as e:
        print(f"âŒ Failed to load {model_name}: {e}")
        return

    load_time = time.time() - start_load
    print(f"â±ï¸  Load Time: {load_time:.2f}s")
    
    # 2. Warmup
    try:
        reranker.compute_score(["test", "test"])
    except Exception as e:
        print(f"âš ï¸  Warmup warning: {e}")
    
    # 3. Inference Benchmark
    total_docs = 0
    total_time = 0
    
    print("\nğŸ” Relevance Scores:")
    for i, case in enumerate(TEST_DATA):
        query = case["query"]
        docs = case["docs"]
        pairs = [[query, doc] for doc in docs]
        
        start_inf = time.time()
        scores = reranker.compute_score(pairs)
        duration = time.time() - start_inf
        
        total_time += duration
        total_docs += len(docs)
        
        print(f"  Query {i+1}: ({duration*1000:.1f}ms / {len(docs)} docs)")
        
        # Handle different output formats (list or single float)
        if not isinstance(scores, list):
            scores = [scores]
            
        scored_docs = list(zip(docs, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        top_doc = scored_docs[0]
        print(f"    â­ Top Result ({top_doc[1]:.4f}): {top_doc[0][:50]}...")

    avg_latency = (total_time / total_docs) * 1000
    print(f"\nâš¡ Average Latency: {avg_latency:.2f} ms/doc")
    print(f"{'='*50}")

if __name__ == "__main__":
    print("ğŸ–¥ï¸  System: CPU Inference Check")
    for model in MODELS:
        benchmark_model(model)

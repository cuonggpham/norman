"""
Comprehensive test for data gaps with various question styles.
Tests 40+ questions with different phrasings.
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from app.db.qdrant import get_qdrant_client, get_collection_name, search
from app.services.embedding import EmbeddingService
from app.llm.query_translator import QueryTranslator
from app.llm.openai_provider import OpenAIProvider

# 48 c√¢u h·ªèi t·∫≠p trung v√†o LU·∫¨T T√ÄI CH√çNH NH·∫¨T B·∫¢N cho ng∆∞·ªùi Vi·ªát Nam
TEST_QUESTIONS = [
    # === THU·∫æ THU NH·∫¨P C√Å NH√ÇN (6 c√¢u) ===
    "Thu·∫ø thu nh·∫≠p c√° nh√¢n ·ªü Nh·∫≠t t√≠nh nh∆∞ th·∫ø n√†o?",
    "Ng∆∞·ªùi n∆∞·ªõc ngo√†i ·ªü Nh·∫≠t c√≥ ph·∫£i ƒë√≥ng thu·∫ø kh√¥ng?",
    "Khai thu·∫ø h√†ng nƒÉm (Á¢∫ÂÆöÁî≥Âëä) ·ªü Nh·∫≠t l√†m th·∫ø n√†o?",
    "C√°c kho·∫£n ƒë∆∞·ª£c kh·∫•u tr·ª´ thu·∫ø thu nh·∫≠p l√† g√¨?",
    "Thu·∫ø su·∫•t thu nh·∫≠p theo t·ª´ng m·ª©c thu nh·∫≠p?",
    "C∆∞ tr√∫ thu·∫ø (Á®éÂãô‰∏ä„ÅÆÂ±Ö‰ΩèËÄÖ) l√† g√¨?",
    
    # === THU·∫æ TI√äU D√ôNG V√Ä THU·∫æ KH√ÅC (6 c√¢u) ===
    "Thu·∫ø ti√™u d√πng (Ê∂àË≤ªÁ®é) ·ªü Nh·∫≠t l√† bao nhi√™u ph·∫ßn trƒÉm?",
    "Thu·∫ø b·∫•t ƒë·ªông s·∫£n (Âõ∫ÂÆöË≥áÁî£Á®é) ƒë∆∞·ª£c t√≠nh nh∆∞ th·∫ø n√†o?",
    "Thu·∫ø qu√† t·∫∑ng (Ë¥à‰∏éÁ®é) quy ƒë·ªãnh ra sao?",
    "Thu·∫ø th·ª´a k·∫ø (Áõ∏Á∂öÁ®é) ·ªü Nh·∫≠t th·∫ø n√†o?",
    "Thu·∫ø doanh nghi·ªáp (Ê≥ï‰∫∫Á®é) l√† bao nhi√™u?",
    "Thu·∫ø √¥ t√¥ v√† ph∆∞∆°ng ti·ªán giao th√¥ng?",
    
    # === NG√ÇN H√ÄNG V√Ä T√ÄI KHO·∫¢N (6 c√¢u) ===
    "Ng∆∞·ªùi n∆∞·ªõc ngo√†i m·ªü t√†i kho·∫£n ng√¢n h√†ng Nh·∫≠t th·∫ø n√†o?",
    "ƒêi·ªÅu ki·ªán m·ªü t√†i kho·∫£n ng√¢n h√†ng ·ªü Nh·∫≠t l√† g√¨?",
    "Chuy·ªÉn ti·ªÅn t·ª´ Nh·∫≠t v·ªÅ Vi·ªát Nam l√†m sao?",
    "Gi·ªõi h·∫°n chuy·ªÉn ti·ªÅn qu·ªëc t·∫ø l√† bao nhi√™u?",
    "Quy ƒë·ªãnh v·ªÅ giao d·ªãch ti·ªÅn m·∫∑t l·ªõn?",
    "M·ªü t√†i kho·∫£n ti·∫øt ki·ªám ·ªü Nh·∫≠t nh∆∞ th·∫ø n√†o?",
    
    # === ƒê·∫¶U T∆Ø V√Ä CH·ª®NG KHO√ÅN (6 c√¢u) ===
    "Ng∆∞·ªùi n∆∞·ªõc ngo√†i c√≥ th·ªÉ ƒë·∫ßu t∆∞ ch·ª©ng kho√°n Nh·∫≠t kh√¥ng?",
    "Thu·∫ø tr√™n l·ª£i nhu·∫≠n ƒë·∫ßu t∆∞ („Ç≠„É£„Éî„Çø„É´„Ç≤„Ç§„É≥Á®é) l√† bao nhi√™u?",
    "T√†i kho·∫£n NISA l√† g√¨ v√† l·ª£i √≠ch c·ªßa n√≥?",
    "iDeCo (ÂÄã‰∫∫ÂûãÁ¢∫ÂÆöÊã†Âá∫Âπ¥Èáë) l√† g√¨?",
    "Thu·∫ø c·ªï t·ª©c (ÈÖçÂΩìÁ®é) ·ªü Nh·∫≠t th·∫ø n√†o?",
    "M·ªü t√†i kho·∫£n ch·ª©ng kho√°n ·ªü Nh·∫≠t c·∫ßn g√¨?",
    
    # === B·∫¢O HI·ªÇM X√É H·ªòI V√Ä H∆ØU TR√ç (6 c√¢u) ===
    "B·∫£o hi·ªÉm x√£ h·ªôi Nh·∫≠t B·∫£n bao g·ªìm nh·ªØng g√¨?",
    "B·∫£o hi·ªÉm y t·∫ø qu·ªëc d√¢n (ÂõΩÊ∞ëÂÅ•Â∫∑‰øùÈô∫) l√† g√¨?",
    "B·∫£o hi·ªÉm h∆∞u tr√≠ qu·ªëc d√¢n (ÂõΩÊ∞ëÂπ¥Èáë) ·ªü Nh·∫≠t th·∫ø n√†o?",
    "B·∫£o hi·ªÉm h∆∞u tr√≠ ng∆∞·ªùi lao ƒë·ªông (ÂéöÁîüÂπ¥Èáë) l√† g√¨?",
    "Lump-sum withdrawal payment (ËÑ±ÈÄÄ‰∏ÄÊôÇÈáë) cho ng∆∞·ªùi r·ªùi Nh·∫≠t l√† g√¨?",
    "ƒêi·ªÅu ki·ªán nh·∫≠n l∆∞∆°ng h∆∞u ·ªü Nh·∫≠t?",
    
    # === T√çN D·ª§NG V√Ä VAY V·ªêN (6 c√¢u) ===
    "Ng∆∞·ªùi n∆∞·ªõc ngo√†i vay mua nh√† ·ªü Nh·∫≠t ƒë∆∞·ª£c kh√¥ng?",
    "ƒêi·ªÅu ki·ªán vay ng√¢n h√†ng (‰ΩèÂÆÖ„É≠„Éº„É≥) ·ªü Nh·∫≠t l√† g√¨?",
    "Th·∫ª t√≠n d·ª•ng cho ng∆∞·ªùi n∆∞·ªõc ngo√†i ·ªü Nh·∫≠t?",
    "L√£i su·∫•t vay mua nh√† ·ªü Nh·∫≠t l√† bao nhi√™u?",
    "Vay ti√™u d√πng (Ê∂àË≤ªËÄÖ„É≠„Éº„É≥) ·ªü Nh·∫≠t th·∫ø n√†o?",
    "Gi·ªõi h·∫°n vay ƒë·ªëi v·ªõi ng∆∞·ªùi n∆∞·ªõc ngo√†i?",
    
    # === TI·ªÄN ƒêI·ªÜN T·ª¨ V√Ä T√ÄI S·∫¢N K·ª∏ THU·∫¨T S·ªê (4 c√¢u) ===
    "Thu·∫ø ti·ªÅn ƒëi·ªán t·ª≠ (‰ªÆÊÉ≥ÈÄöË≤®) ·ªü Nh·∫≠t nh∆∞ th·∫ø n√†o?",
    "Quy ƒë·ªãnh v·ªÅ giao d·ªãch Bitcoin ·ªü Nh·∫≠t?",
    "Khai b√°o l·ª£i nhu·∫≠n t·ª´ crypto?",
    "S√†n giao d·ªãch ti·ªÅn ƒëi·ªán t·ª≠ h·ª£p ph√°p ·ªü Nh·∫≠t?",
    
    # === B·∫§T ƒê·ªòNG S·∫¢N V√Ä ƒê·∫¶U T∆Ø NH√Ä ƒê·∫§T (4 c√¢u) ===
    "Ng∆∞·ªùi n∆∞·ªõc ngo√†i c√≥ th·ªÉ mua nh√† ·ªü Nh·∫≠t kh√¥ng?",
    "Thu·∫ø khi mua b√°n b·∫•t ƒë·ªông s·∫£n ·ªü Nh·∫≠t?",
    "Chi ph√≠ mua nh√† ·ªü Nh·∫≠t bao g·ªìm nh·ªØng g√¨?",
    "Quy ƒë·ªãnh v·ªÅ s·ªü h·ªØu ƒë·∫•t ƒëai cho ng∆∞·ªùi n∆∞·ªõc ngo√†i?",
    
    # === KINH DOANH V√Ä KH·ªûI NGHI·ªÜP (4 c√¢u) ===
    "Th√†nh l·∫≠p c√¥ng ty ·ªü Nh·∫≠t c·∫ßn nh·ªØng g√¨?",
    "Thu·∫ø doanh nghi·ªáp nh·ªè (ÂÄã‰∫∫‰∫ãÊ•≠‰∏ª) th·∫ø n√†o?",
    "Visa kinh doanh (ÁµåÂñ∂ÁÆ°ÁêÜ„Éì„Ç∂) y√™u c·∫ßu g√¨?",
    "Quy ƒë·ªãnh v·ªÅ v·ªën t·ªëi thi·ªÉu th√†nh l·∫≠p c√¥ng ty?",
]

def test_comprehensive():
    print("=" * 65)
    print("COMPREHENSIVE DATA GAP TEST (40+ Questions)")
    print("=" * 65)
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("ERROR: OPENAI_API_KEY not set")
        return
    
    embedding = EmbeddingService(api_key=api_key)
    llm = OpenAIProvider(api_key=api_key)
    translator = QueryTranslator(llm=llm)
    client = get_qdrant_client()
    collection_name = get_collection_name()
    
    results = []
    
    for i, q in enumerate(TEST_QUESTIONS):
        print(f"\n[{i+1}/{len(TEST_QUESTIONS)}] {q[:40]}...")
        
        # Translate if Vietnamese
        ja_query = translator.translate(q)
        
        # Search
        query_vec = embedding.embed_text(ja_query)
        hits = search(client, query_vec, top_k=5, collection_name=collection_name)
        
        if hits:
            scores = [h["score"] for h in hits]
            laws = set(h["payload"].get("law_title", "?") for h in hits)
            top = scores[0]
            
            if top >= 0.6: status = "‚úÖ"
            elif top >= 0.4: status = "‚ö†Ô∏è"
            else: status = "‚ùå"
            
            print(f"   {status} Top: {top:.3f} | {list(laws)[0][:15]}...")
            results.append({"q": q, "top": top, "laws": list(laws), "status": status})
        else:
            print("   ‚ùå NO RESULTS")
            results.append({"q": q, "top": 0, "laws": [], "status": "‚ùå"})
    
    # Summary
    print("\n" + "=" * 65)
    print("SUMMARY")
    print("=" * 65)
    
    good = [r for r in results if r["status"] == "‚úÖ"]
    warn = [r for r in results if r["status"] == "‚ö†Ô∏è"]
    bad = [r for r in results if r["status"] == "‚ùå"]
    
    print(f"\n‚úÖ Good (‚â•0.6): {len(good)} ({len(good)*100//len(results)}%)")
    print(f"‚ö†Ô∏è  Medium (0.4-0.6): {len(warn)} ({len(warn)*100//len(results)}%)")
    print(f"‚ùå Poor (<0.4): {len(bad)} ({len(bad)*100//len(results)}%)")
    
    # Group by topic
    print("\nüìä BY TOPIC:")
    topics = {
        # 48 c√¢u h·ªèi lu·∫≠t t√†i ch√≠nh (9 topics)
        "Thu·∫ø thu nh·∫≠p": results[0:6],
        "Thu·∫ø kh√°c": results[6:12],
        "Ng√¢n h√†ng": results[12:18],
        "ƒê·∫ßu t∆∞": results[18:24],
        "B·∫£o hi·ªÉm XH": results[24:30],
        "T√≠n d·ª•ng": results[30:36],
        "Ti·ªÅn ƒëi·ªán t·ª≠": results[36:40],
        "B·∫•t ƒë·ªông s·∫£n": results[40:44],
        "Kinh doanh": results[44:48],
    }
    
    for topic, items in topics.items():
        if items:
            avg = sum(r["top"] for r in items) / len(items)
            status = "‚úÖ" if avg >= 0.6 else ("‚ö†Ô∏è" if avg >= 0.4 else "‚ùå")
            print(f"   {status} {topic}: avg={avg:.3f}")
    
    if bad:
        print("\n‚ùå POOR QUESTIONS:")
        for r in bad:
            print(f"   - {r['q'][:45]}... ({r['top']:.3f})")

if __name__ == "__main__":
    test_comprehensive()

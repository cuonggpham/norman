# BÃO CÃO Äá»’ ÃN Tá»T NGHIá»†P

## Há»† THá»NG RAG TÆ¯ Váº¤N PHÃP LUáº¬T TÃ€I CHÃNH NHáº¬T Báº¢N CHO NGÆ¯á»œI VIá»†T NAM

---

**Sinh viÃªn thá»±c hiá»‡n:** [Há» vÃ  tÃªn]  
**MÃ£ sinh viÃªn:** [MÃ£ sá»‘]  
**Giáº£ng viÃªn hÆ°á»›ng dáº«n:** [Há» vÃ  tÃªn]  
**NÄƒm há»c:** 2025-2026

---

## Má»¤C Lá»¤C

1. [Giá»›i thiá»‡u](#1-giá»›i-thiá»‡u)
2. [CÆ¡ sá»Ÿ lÃ½ thuyáº¿t](#2-cÆ¡-sá»Ÿ-lÃ½-thuyáº¿t)
3. [PhÃ¢n tÃ­ch vÃ  thiáº¿t káº¿ há»‡ thá»‘ng](#3-phÃ¢n-tÃ­ch-vÃ -thiáº¿t-káº¿-há»‡-thá»‘ng)
4. [Triá»ƒn khai há»‡ thá»‘ng RAG](#4-triá»ƒn-khai-há»‡-thá»‘ng-rag)
5. [Thá»­ nghiá»‡m vÃ  Ä‘Ã¡nh giÃ¡](#5-thá»­-nghiá»‡m-vÃ -Ä‘Ã¡nh-giÃ¡)
6. [Káº¿t luáº­n vÃ  hÆ°á»›ng phÃ¡t triá»ƒn](#6-káº¿t-luáº­n-vÃ -hÆ°á»›ng-phÃ¡t-triá»ƒn)
7. [TÃ i liá»‡u tham kháº£o](#7-tÃ i-liá»‡u-tham-kháº£o)

---

## 1. GIá»šI THIá»†U

### 1.1. Äáº·t váº¥n Ä‘á»

Cá»™ng Ä‘á»“ng ngÆ°á»i Viá»‡t Nam sinh sá»‘ng vÃ  lÃ m viá»‡c táº¡i Nháº­t Báº£n ngÃ y cÃ ng Ä‘Ã´ng Ä‘áº£o, vá»›i nhu cáº§u ráº¥t lá»›n vá» tÃ¬m hiá»ƒu cÃ¡c quy Ä‘á»‹nh phÃ¡p luáº­t tÃ i chÃ­nh cá»§a Nháº­t Báº£n. Tuy nhiÃªn, há» gáº·p pháº£i nhiá»u rÃ o cáº£n:

- **RÃ o cáº£n ngÃ´n ngá»¯**: CÃ¡c vÄƒn báº£n phÃ¡p luáº­t Ä‘Æ°á»£c viáº¿t báº±ng tiáº¿ng Nháº­t vá»›i thuáº­t ngá»¯ phÃ¡p lÃ½ phá»©c táº¡p
- **KhÃ³ khÄƒn tiáº¿p cáº­n**: Nguá»“n thÃ´ng tin phÃ¢n tÃ¡n, khÃ³ tÃ¬m kiáº¿m trÃªn nhiá»u trang web khÃ¡c nhau
- **Thiáº¿u há»— trá»£ song ngá»¯**: Háº§u háº¿t cÃ¡c cÃ´ng cá»¥ tÃ¬m kiáº¿m phÃ¡p luáº­t khÃ´ng há»— trá»£ tiáº¿ng Viá»‡t

### 1.2. Má»¥c tiÃªu Ä‘á»“ Ã¡n

XÃ¢y dá»±ng há»‡ thá»‘ng **RAG (Retrieval-Augmented Generation)** há»— trá»£ ngÆ°á»i Viá»‡t Nam tra cá»©u vÃ  tÆ° váº¥n phÃ¡p luáº­t tÃ i chÃ­nh Nháº­t Báº£n:

1. **TÃ¬m kiáº¿m ngá»¯ nghÄ©a xuyÃªn ngÃ´n ngá»¯**: Cho phÃ©p ngÆ°á»i dÃ¹ng há»i báº±ng tiáº¿ng Viá»‡t, tÃ¬m kiáº¿m trong kho vÄƒn báº£n tiáº¿ng Nháº­t
2. **Tráº£ lá»i chÃ­nh xÃ¡c cÃ³ trÃ­ch dáº«n**: Sinh cÃ¢u tráº£ lá»i tiáº¿ng Viá»‡t kÃ¨m nguá»“n gá»‘c Ä‘iá»u luáº­t
3. **Self-correction**: Tá»± Ä‘á»™ng cáº£i thiá»‡n káº¿t quáº£ khi tÃ i liá»‡u khÃ´ng Ä‘á»§ cháº¥t lÆ°á»£ng

### 1.3. Pháº¡m vi Ä‘á»“ Ã¡n

**Pháº¡m vi tÆ° váº¥n phÃ¡p luáº­t:**
- ðŸ’° **Thuáº¿ (ç¨Žé‡‘)**: Thuáº¿ thu nháº­p, thuáº¿ tiÃªu dÃ¹ng, thuáº¿ cÆ° trÃº, khai thuáº¿ cuá»‘i nÄƒm (ç¢ºå®šç”³å‘Š)
- ðŸ¥ **Báº£o hiá»ƒm xÃ£ há»™i (ç¤¾ä¼šä¿é™º)**: Báº£o hiá»ƒm y táº¿, lÆ°Æ¡ng hÆ°u, báº£o hiá»ƒm tháº¥t nghiá»‡p
- ðŸ“ˆ **Äáº§u tÆ° & Tiáº¿t kiá»‡m**: NISA, iDeCo, ãµã‚‹ã•ã¨ç´ç¨Ž
- ðŸ’µ **TÃ i chÃ­nh cÃ¡ nhÃ¢n**: Chuyá»ƒn tiá»n quá»‘c táº¿, thuáº¿ cho ngÆ°á»i nÆ°á»›c ngoÃ i

**Nguá»“n dá»¯ liá»‡u:**
- **233 vÄƒn báº£n luáº­t** tá»« e-Gov API (Cá»•ng thÃ´ng tin phÃ¡p luáº­t Nháº­t Báº£n)
- **15,629 chunks** sau khi phÃ¢n Ä‘oáº¡n vÃ  xá»­ lÃ½

---

## 2. CÆ  Sá»ž LÃ THUYáº¾T

### 2.1. Retrieval-Augmented Generation (RAG)

RAG lÃ  má»™t kiáº¿n trÃºc káº¿t há»£p giá»¯a Information Retrieval vÃ  Large Language Model, giáº£i quyáº¿t váº¥n Ä‘á» "hallucination" cá»§a LLM báº±ng cÃ¡ch cung cáº¥p context thá»±c táº¿.

```mermaid
flowchart LR
    A[Query\nTiáº¿ng Viá»‡t] --> B[Query\nTranslation]
    B --> C[Embedding]
    C --> D[Vector\nSearch]
    D --> E[Retrieved\nChunks]
    E --> F[LLM\nGeneration]
    F --> G[Answer\nTiáº¿ng Viá»‡t]
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
```

**Æ¯u Ä‘iá»ƒm cá»§a RAG:**
- Giáº£m thiá»ƒu hallucination báº±ng cÃ¡ch grounding vÃ o dá»¯ liá»‡u thá»±c
- CÃ³ thá»ƒ cáº­p nháº­t kiáº¿n thá»©c mÃ  khÃ´ng cáº§n fine-tuning model
- Cung cáº¥p nguá»“n trÃ­ch dáº«n minh báº¡ch

### 2.2. Vector Embedding vÃ  Semantic Search

**Embedding** lÃ  ká»¹ thuáº­t biáº¿n Ä‘á»•i vÄƒn báº£n thÃ nh vector sá»‘ trong khÃ´ng gian Ä‘a chiá»u, báº£o toÃ n ngá»¯ nghÄ©a.

```mermaid
flowchart TB
    subgraph Embedding["Vector Embedding Process"]
        T1["åŠ´åƒåŸºæº–æ³•ç¬¬ä¸€æ¡"] --> E1["OpenAI API"]
        E1 --> V1["[0.12, -0.34, 0.56, ..., 0.89]"]
        V1 --> |3072 dimensions| DB[(Qdrant)]
    end
    
    subgraph Search["Semantic Search"]
        Q["Query: Thá»i gian lÃ m viá»‡c?"] --> QE["Query Embedding"]
        QE --> CS["Cosine Similarity"]
        DB --> CS
        CS --> R["Top-K Results"]
    end
```

**MÃ´ hÃ¬nh sá»­ dá»¥ng:** `text-embedding-3-large` cá»§a OpenAI
- **KÃ­ch thÆ°á»›c vector:** 3072 dimensions
- **Æ¯u Ä‘iá»ƒm:** Hiá»‡u quáº£ cao vá»›i vÄƒn báº£n Ä‘a ngá»¯, Ä‘áº·c biá»‡t lÃ  Nháº­t-Viá»‡t

### 2.3. Hybrid Search

Káº¿t há»£p **Vector Search** (Dense) vÃ  **BM25 Search** (Sparse) Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai:

```mermaid
flowchart TB
    Q[Query] --> DE[Dense Embedding]
    Q --> SE[Sparse Embedding\nBM25]
    
    DE --> VS[Vector Search]
    SE --> KS[Keyword Search]
    
    VS --> |Semantic Match| RRF[Reciprocal Rank\nFusion]
    KS --> |Exact Match| RRF
    
    RRF --> FR[Final Ranking]
```

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|-------------|---------|------------|
| Vector Search | Hiá»ƒu ngá»¯ nghÄ©a, tÃ¬m synonym | CÃ³ thá»ƒ miss keyword chÃ­nh xÃ¡c |
| BM25/Sparse | ChÃ­nh xÃ¡c vá»›i keyword | KhÃ´ng hiá»ƒu ngá»¯ nghÄ©a |
| **Hybrid** | Káº¿t há»£p cáº£ hai | Cáº§n tuning fusion weight |

**CÃ´ng thá»©c Reciprocal Rank Fusion (RRF):**
$$RRF_{score}(d) = \sum_{q \in Q} \frac{1}{k + rank(d, q)}$$

Trong Ä‘Ã³ `k = 60` lÃ  háº±ng sá»‘ smoothing.

### 2.4. Two-Stage Retrieval vá»›i Reranking

Pipeline hai giai Ä‘oáº¡n káº¿t há»£p bi-encoder nhanh vÃ  cross-encoder chÃ­nh xÃ¡c:

```mermaid
flowchart LR
    Q[Query] --> BE[Bi-Encoder\nFast Retrieval]
    BE --> |Top 20-40| CE[Cross-Encoder\nBGE Reranker]
    CE --> |Top 5| R[Final Results]
    
    subgraph Stage1 [Stage 1: Recall]
        BE
    end
    
    subgraph Stage2 [Stage 2: Precision]
        CE
    end
```

**Bi-Encoder (Stage 1):**
- Encode query vÃ  document Ä‘á»™c láº­p
- Nhanh: O(1) vá»›i pre-computed document embeddings
- KÃ©m chÃ­nh xÃ¡c hÆ¡n do khÃ´ng cÃ³ cross-attention

**Cross-Encoder (Stage 2):**
- Encode query-document pair cÃ¹ng nhau
- Cháº­m: O(n) vá»›i má»—i document
- ChÃ­nh xÃ¡c hÆ¡n do cÃ³ full attention

**MÃ´ hÃ¬nh reranker:** `BAAI/bge-reranker-large`

### 2.5. LangGraph Agent Framework

LangGraph cho phÃ©p xÃ¢y dá»±ng workflow phá»©c táº¡p vá»›i:
- **Stateful execution**: LÆ°u tráº¡ng thÃ¡i qua cÃ¡c bÆ°á»›c
- **Conditional edges**: Routing dá»±a trÃªn Ä‘iá»u kiá»‡n
- **Self-correction loop**: Tá»± Ä‘á»™ng retry khi káº¿t quáº£ khÃ´ng tá»‘t

```mermaid
stateDiagram-v2
    [*] --> Translate
    Translate --> Retrieve
    Retrieve --> Grade
    Grade --> Rerank: relevant >= 2
    Grade --> Rewrite: relevant < 2
    Rewrite --> Retrieve: retry < 2
    Rewrite --> Rerank: retry >= 2
    Rerank --> Generate
    Generate --> [*]
```

---

## 3. PHÃ‚N TÃCH VÃ€ THIáº¾T Káº¾ Há»† THá»NG

### 3.1. Kiáº¿n trÃºc tá»•ng quan

```mermaid
flowchart TB
    subgraph Client["Client Layer"]
        UI[Web Interface]
        CLI[CLI Tools]
    end
    
    subgraph API["API Layer"]
        FE[FastAPI Server]
        FE --> |/api/chat| RAG
        FE --> |/api/search| RAG
    end
    
    subgraph Core["RAG Core"]
        RAG[RAG Pipeline]
        RAG --> QT[Query Translator]
        RAG --> EMB[Embedding Service]
        RAG --> VS[Vector Store]
        RAG --> RR[Reranker]
        RAG --> LLM[LLM Provider]
    end
    
    subgraph Agent["LangGraph Agent"]
        AG[Legal RAG Agent]
        AG --> TN[Translate Node]
        AG --> RN[Retrieve Node]
        AG --> GN[Grade Node]
        AG --> REN[Rerank Node]
        AG --> GEN[Generate Node]
        AG --> RWN[Rewrite Node]
    end
    
    subgraph External["External Services"]
        QDRANT[(Qdrant Cloud)]
        OPENAI[OpenAI API]
        EGOV[e-Gov API]
    end
    
    UI --> FE
    CLI --> FE
    EMB --> OPENAI
    LLM --> OPENAI
    VS --> QDRANT
```

### 3.2. Cáº¥u trÃºc thÆ° má»¥c dá»± Ã¡n

```
norman/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/            # LangGraph Agent
â”‚   â”‚   â”‚   â”œâ”€â”€ graph.py       # StateGraph definition
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py       # 6 agent nodes
â”‚   â”‚   â”‚   â””â”€â”€ state.py       # TypedDict state
â”‚   â”‚   â”œâ”€â”€ api/               # REST API
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py      # Endpoints
â”‚   â”‚   â”‚   â””â”€â”€ deps.py        # Dependency injection
â”‚   â”‚   â”œâ”€â”€ core/              # Configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Settings
â”‚   â”‚   â”‚   â””â”€â”€ protocols.py   # Abstract interfaces
â”‚   â”‚   â”œâ”€â”€ db/                # Database layer
â”‚   â”‚   â”‚   â””â”€â”€ qdrant.py      # Qdrant client
â”‚   â”‚   â”œâ”€â”€ llm/               # LLM modules
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py        # Abstract LLM
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query_translator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query_analyzer.py
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚   â”œâ”€â”€ pipelines/         # RAG orchestration
â”‚   â”‚   â”‚   â””â”€â”€ rag.py         # RAGPipeline class
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py
â”‚   â”‚   â”‚   â”œâ”€â”€ reranker.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sparse_embedding.py
â”‚   â”‚   â”‚   â””â”€â”€ search.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ scripts/               # Data pipeline
â”‚       â”œâ”€â”€ downloader.py      # e-Gov API
â”‚       â”œâ”€â”€ xml_parser.py      # XML â†’ JSON
â”‚       â”œâ”€â”€ chunker.py         # Smart chunking
â”‚       â”œâ”€â”€ embedder.py        # Batch embedding
â”‚       â”œâ”€â”€ indexer.py         # Vector upload
â”‚       â””â”€â”€ hybrid_indexer.py  # Hybrid indexing
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ raw/                   # XML files
â”‚   â”œâ”€â”€ processed/             # JSON files
â”‚   â”œâ”€â”€ chunks/                # Chunked data
â”‚   â””â”€â”€ embeddings/            # Cached vectors
â””â”€â”€ docs/
    â””â”€â”€ REPORT.md
```

### 3.3. Data Processing Pipeline

Data pipeline xá»­ lÃ½ dá»¯ liá»‡u tá»« nguá»“n e-Gov API Ä‘áº¿n vector database Qdrant vá»›i 5 giai Ä‘oáº¡n chÃ­nh:

```mermaid
flowchart TB
    subgraph Stage1["Giai Ä‘oáº¡n 1: Thu tháº­p dá»¯ liá»‡u"]
        API[("e-Gov API\nlaws.e-gov.go.jp")] --> DL[downloader.py]
        DL --> |"233 files"| XML[(data/xml_raw/*.xml)]
    end
    
    subgraph Stage2["Giai Ä‘oáº¡n 2: Parse XML"]
        XML --> XP[xml_parser.py]
        XP --> |"Hierarchical JSON"| JSON[(data/processed/*.json)]
    end
    
    subgraph Stage3["Giai Ä‘oáº¡n 3: Chunking"]
        JSON --> CH[chunker.py]
        CH --> |"15,629 chunks"| CK[(data/chunks/*.json)]
    end
    
    subgraph Stage4["Giai Ä‘oáº¡n 4: Embedding"]
        CK --> EM[embedder.py]
        EM --> |"Dense vectors"| EMB[(data/embeddings/*.npy)]
    end
    
    subgraph Stage5["Giai Ä‘oáº¡n 5: Indexing"]
        EMB --> IX[hybrid_indexer.py]
        CK --> |"BM25 sparse"| IX
        IX --> |"Upload"| QD[("Qdrant Cloud\njapanese_laws_hybrid")]
    end
    
    style Stage1 fill:#e3f2fd
    style Stage2 fill:#e8f5e9
    style Stage3 fill:#fff3e0
    style Stage4 fill:#fce4ec
    style Stage5 fill:#f3e5f5
```

---

#### 3.3.1. Thu tháº­p dá»¯ liá»‡u (downloader.py)

**Nguá»“n dá»¯ liá»‡u:** e-Gov Laws API (https://laws.e-gov.go.jp/api/2)

```mermaid
flowchart TB
    subgraph Search["TÃ¬m kiáº¿m luáº­t liÃªn quan"]
        C1[Category Search] --> |"å›½ç¨Ž, åœ°æ–¹è²¡æ”¿, ç¤¾ä¼šä¿é™º, åŠ´åƒ"| R1[Results]
        C2[Keyword Search] --> |"å¤–å›½äºº, æ‰€å¾—ç¨Ž, å¹´é‡‘, å¥åº·ä¿é™º"| R2[Results]
        R1 --> DD[Deduplicate]
        R2 --> DD
    end
    
    subgraph Filter["Lá»c cháº¥t lÆ°á»£ng"]
        DD --> F1{Modern Era?\nShowa+}
        F1 --> |No| SKIP1[Skip]
        F1 --> |Yes| F2{Law Type?\nAct/CabinetOrder}
        F2 --> |No| SKIP2[Skip]
        F2 --> |Yes| F3{Status?\nCurrentEnforced}
        F3 --> |No| SKIP3[Skip]
        F3 --> |Yes| PASS[Pass Filter]
    end
    
    subgraph Download["Download XML"]
        PASS --> DL[Download XML]
        DL --> |"Rate limit 1.2s"| SAVE[(data/xml_raw/)]
    end
```

**API Endpoints sá»­ dá»¥ng:**

| Endpoint | Method | Má»¥c Ä‘Ã­ch |
|----------|--------|----------|
| `/laws` | GET | Láº¥y danh sÃ¡ch luáº­t theo category |
| `/keyword` | GET | TÃ¬m kiáº¿m luáº­t theo keyword |
| `/law_data/{law_id}` | GET | Download ná»™i dung XML cá»§a luáº­t |

**Financial Categories thu tháº­p:**
```python
FINANCIAL_CATEGORIES = [
    "å›½ç¨Ž",           # National Tax (Income tax, consumption tax)
    "åœ°æ–¹è²¡æ”¿",       # Local Finance (Resident tax)
    "ç¤¾ä¼šä¿é™º",       # Social Insurance (Pension, health insurance)
    "åŠ´åƒ",           # Labor (Labor standards, employment)
]

FOREIGNER_KEYWORDS = [
    "å¤–å›½äºº",         # Foreigner
    "åœ¨ç•™",           # Residence status
    "æ‰€å¾—ç¨Ž",         # Income tax
    "å¹´é‡‘",           # Pension
    "å¥åº·ä¿é™º",       # Health insurance
    "ä½æ°‘ç¨Ž",         # Resident tax
]
```

**Quality Filters:**
```python
# Chá»‰ láº¥y luáº­t tá»« thá»i Showa trá»Ÿ Ä‘i (1926+)
MODERN_ERAS = ["Showa", "Heisei", "Reiwa"]

# Loáº¡i luáº­t Ä‘Æ°á»£c phÃ©p
ALLOWED_LAW_TYPES = ["Act", "CabinetOrder"]

# Chá»‰ láº¥y luáº­t Ä‘ang cÃ³ hiá»‡u lá»±c
VALID_STATUS = ["CurrentEnforced"]
```

**Output Statistics:**
- **Total laws searched:** ~500+ 
- **After filtering:** 233 laws
- **File format:** XML (e-Gov standard format)

---

#### 3.3.2. XML Parser (xml_parser.py)

Parse cáº¥u trÃºc XML phÃ¡p luáº­t Nháº­t Báº£n thÃ nh JSON cÃ³ cáº¥u trÃºc hierarchy.

```mermaid
flowchart TB
    subgraph XMLStructure["Cáº¥u trÃºc XML e-Gov"]
        ROOT[law_data] --> LI[law_info]
        ROOT --> RI[revision_info]
        ROOT --> LFT[law_full_text]
        LFT --> LAW[Law]
        LAW --> LB[LawBody]
        LB --> TOC[TOC]
        LB --> MP[MainProvision]
        LB --> SP[SupplProvision]
        MP --> CH[Chapter]
        CH --> ART[Article]
        ART --> PARA[Paragraph]
        PARA --> SENT[Sentence]
        PARA --> ITEM[Item]
    end
    
    subgraph JSONOutput["JSON Output"]
        J1[law_info]
        J2[revision_info]
        J3["law_full_text.law_body"]
        J3 --> J4[chapters]
        J4 --> J5[articles]
        J5 --> J6[paragraphs]
        J6 --> J7[sentences/items]
    end
    
    XMLStructure --> |"parse_xml_file()"| JSONOutput
```

**Extracted Metadata:**

| Field | Source | Example |
|-------|--------|---------|
| `law_id` | law_info | "411AC0000000073" |
| `law_title` | revision_info | "æ‰€å¾—ç¨Žæ³•" |
| `law_title_kana` | revision_info | "ã‚·ãƒ§ãƒˆã‚¯ã‚¼ã‚¤ãƒ›ã‚¦" |
| `abbrev` | revision_info | "æ‰€ç¨Ž" |
| `category` | revision_info | "å›½ç¨Ž" |
| `law_type` | law_info | "Act" |
| `promulgation_date` | law_info | "1965-03-31" |
| `current_revision_status` | revision_info | "CurrentEnforced" |

**Hierarchical Structure Output:**
```json
{
  "law_info": {
    "law_id": "411AC0000000073",
    "law_type": "Act",
    "promulgation_date": "1965-03-31"
  },
  "revision_info": {
    "law_title": "æ‰€å¾—ç¨Žæ³•",
    "category": "å›½ç¨Ž",
    "current_revision_status": "CurrentEnforced"
  },
  "law_full_text": {
    "law_body": {
      "title": {"text": "æ‰€å¾—ç¨Žæ³•", "abbrev": "æ‰€ç¨Ž"},
      "main_provision": {
        "chapters": [
          {
            "num": "ç¬¬ä¸€ç·¨",
            "title": "ç·å‰‡",
            "articles": [
              {
                "num": "1",
                "title": "ç¬¬ä¸€æ¡",
                "caption": "è¶£æ—¨",
                "paragraphs": [
                  {
                    "num": "1",
                    "sentences": [{"num": "1", "text": "..."}]
                  }
                ]
              }
            ]
          }
        ]
      },
      "supplementary_provisions": [...]
    }
  }
}
```

---

#### 3.3.3. Smart Chunking (chunker.py)

PhÃ¢n Ä‘oáº¡n vÄƒn báº£n giá»¯ nguyÃªn context hierarchy, sá»­ dá»¥ng **Paragraph (Khoáº£n)** lÃ m Ä‘Æ¡n vá»‹ chunk.

```mermaid
flowchart TB
    subgraph Input["JSON Law Document"]
        L["Law: åŠ´åƒåŸºæº–æ³•"]
        L --> C1["Chapter: ç¬¬ä¸€ç«  ç·å‰‡"]
        L --> C2["Chapter: ç¬¬äºŒç«  åŠ´åƒå¥‘ç´„"]
        C1 --> A1["Article: ç¬¬ä¸€æ¡ (è¶£æ—¨)"]
        C1 --> A2["Article: ç¬¬äºŒæ¡"]
        A1 --> P1["Paragraph 1"]
        A1 --> P2["Paragraph 2"]
        P1 --> S1["Sentence 1"]
        P1 --> S2["Sentence 2"]
        P1 --> I1["Item ä¸€"]
        P1 --> I2["Item äºŒ"]
    end
    
    subgraph Process["Chunking Process"]
        P1 --> EX["extract_paragraph_text()"]
        EX --> |"Combine sentences + items"| TXT["Raw Text"]
        TXT --> CTX["Add Context:\nlaw_title + article_title"]
        CTX --> META["Add Metadata"]
    end
    
    subgraph Output["Chunk Output"]
        META --> CH1["Chunk Object"]
        CH1 --> ID["chunk_id"]
        CH1 --> T["text"]
        CH1 --> TC["text_with_context"]
        CH1 --> M["metadata"]
        CH1 --> HP["highlight_path"]
    end
```

**Chunking Strategy:**

| Aspect | Decision | Rationale |
|--------|----------|-----------|
| **Unit** | Paragraph (Khoáº£n) | Semantic coherence, legal citations |
| **Context** | Include law_title + article_title | Better embedding quality |
| **Items** | Flatten into paragraph | Keep related content together |
| **Supplementary** | Separate source_type | Distinguish main vsé™„å‰‡ |

**Chunk Data Structure:**
```python
@dataclass
class Chunk:
    chunk_id: str              # "411AC0000000073_1_1"
    text: str                  # Raw paragraph text
    text_with_context: str     # "æ‰€å¾—ç¨Žæ³• ç¬¬ä¸€æ¡ (è¶£æ—¨) ..."
    metadata: ChunkMetadata    # Full metadata for filtering
    char_count: int            # Character count
    token_estimate: int        # ~chars/2 for Japanese
    
    def get_highlight_path(self) -> dict:
        """Path for UI highlighting."""
        return {
            "law": self.metadata.law_title,
            "chapter": self.metadata.chapter_title,
            "article": self.metadata.article_title,
            "paragraph": f"{self.metadata.paragraph_num}é …"
        }
**Chunking Statistics:**

| Metric | Value |
|--------|-------|
| Total Laws | 233 |
| Total Chunks | 15,629 |
| Avg Chunks/Law | 67 |
| Avg Chunk Size | ~800 characters |
| Token Estimate | ~400 tokens/chunk |

---

#### 3.3.4. Embedding (embedder.py)

Táº¡o dense embeddings vá»›i OpenAI API, há»— trá»£ batch processing vÃ  resume capability.

```mermaid
flowchart TB
    subgraph Input["Load Chunks"]
        CK[(data/chunks/)] --> LC[Load Chunks by Law]
        LC --> CH[Chunks List]
    end
    
    subgraph Process["Batch Embedding"]
        CH --> |"text_with_context"| BA[Batch 100 texts]
        BA --> API[OpenAI API]
        API --> |"3072-dim vectors"| VE[Embeddings]
        
        API --> |"Token Overflow"| SP[Split Batch]
        SP --> BA
        
        API --> |"Rate Limit"| RT[Retry with Backoff]
        RT --> BA
    end
    
    subgraph Output["Save Results"]
        VE --> NP[numpy array]
        NP --> SAVE[(data/embeddings/\n{law_id}_embeddings.npy)]
        CH --> META[(data/embeddings/\n{law_id}_chunks.json)]
    end
```

**Embedding Configuration:**
```python
# Model settings
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 3072

# Processing settings
BATCH_SIZE = 100
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds
MAX_TOKENS_PER_TEXT = 6000
```

**Features:**
- **Resume capability**: Skip already processed laws
- **Token overflow handling**: Auto-split large batches
- **Error recovery**: Exponential backoff retry
- **Progress tracking**: Save checkpoint after each law

**Output Statistics:**
- **Total embeddings:** 15,629
- **Dimensions:** 3,072
- **File size:** ~192 MB (.npy files)
- **Processing time:** ~30 minutes

---

#### 3.3.5. Hybrid Indexing (hybrid_indexer.py)

Upload data lÃªn Qdrant vá»›i cáº£ Dense vÃ  Sparse vectors cho hybrid search.

```mermaid
flowchart TB
    subgraph Load["Load Data"]
        EMB[(embeddings/*.npy)] --> DE[Dense Vectors]
        CK[(chunks/*.json)] --> CH[Chunks + Metadata]
    end
    
    subgraph Sparse["Generate Sparse Vectors"]
        CH --> |"text_with_context"| BM25[fastembed BM25]
        BM25 --> SP[Sparse Vectors]
    end
    
    subgraph Collection["Qdrant Collection Setup"]
        COL[japanese_laws_hybrid]
        COL --> DC[Dense Config:\nCosine, 3072-dim]
        COL --> SC[Sparse Config:\nBM25]
    end
    
    subgraph Upload["Batch Upload"]
        DE --> PT[PointStruct]
        SP --> PT
        CH --> |"metadata payload"| PT
        PT --> |"batch 100"| QD[(Qdrant Cloud)]
    end
```

**Collection Configuration:**
```python
# Dense vector config
dense_config = VectorParams(
    size=3072,
    distance=Distance.COSINE,
)

# Sparse vector config (BM25)
sparse_config = SparseVectorParams(
    modifier=Modifier.IDF,  # TF-IDF weighting
)

# Collection vá»›i multi-vector
client.recreate_collection(
    collection_name="japanese_laws_hybrid",
    vectors_config={"dense": dense_config},
    sparse_vectors_config={"sparse": sparse_config},
)
```

**Payload Metadata:**
```python
payload = {
    "chunk_id": chunk["chunk_id"],
    "text": chunk["text"],
    "text_with_context": chunk["text_with_context"],
    "law_id": metadata["law_id"],
    "law_title": metadata["law_title"],
    "law_abbrev": metadata.get("law_abbrev"),
    "category": metadata["category"],
    "article_title": metadata["article_title"],
    "article_caption": metadata.get("article_caption"),
    "chapter_title": metadata.get("chapter_title"),
    "paragraph_num": metadata["paragraph_num"],
    "source_type": metadata["source_type"],
    "highlight_path": chunk["highlight_path"],
}
```

**Memory Optimization:**
- **File-by-file processing**: Avoid loading all data into memory
- **Streaming sparse embedding**: Generate on-the-fly per file
- **Batch upsert**: 100 points per API call

**Final Index Statistics:**

| Metric | Value |
|--------|-------|
| Collection Name | japanese_laws_hybrid |
| Total Points | 15,629 |
| Dense Vectors | 3,072 dimensions |
| Sparse Vectors | BM25 (variable) |
| Payload Indexes | category, law_title |
| Storage | Qdrant Cloud Free Tier |

### 3.4. PhÃ¢n tÃ­ch lÃ½ do lá»±a chá»n phÆ°Æ¡ng phÃ¡p

#### 3.4.1. Lá»±a chá»n nguá»“n dá»¯ liá»‡u

| Quyáº¿t Ä‘á»‹nh | LÃ½ do |
|------------|-------|
| **e-Gov API thay vÃ¬ web scraping** | API chÃ­nh thá»©c Ä‘áº£m báº£o dá»¯ liá»‡u chÃ­nh xÃ¡c, cáº­p nháº­t, vÃ  cÃ³ cáº¥u trÃºc chuáº©n. Web scraping cÃ³ thá»ƒ bá»‹ block vÃ  data khÃ´ng nháº¥t quÃ¡n. |
| **Chá»‰ láº¥y luáº­t tá»« Showa (1926+)** | Luáº­t thá»i Meiji/Taisho thÆ°á»ng Ä‘Ã£ lá»—i thá»i hoáº·c bá»‹ thay tháº¿. NgÃ´n ngá»¯ phÃ¡p lÃ½ cÅ© khÃ³ xá»­ lÃ½ vÃ  Ã­t relevant cho use case hiá»‡n Ä‘áº¡i. |
| **Filter CurrentEnforced only** | Luáº­t Ä‘Ã£ bá»‹ bÃ£i bá» hoáº·c sá»­a Ä‘á»•i khÃ´ng nÃªn xuáº¥t hiá»‡n trong káº¿t quáº£ tÆ° váº¥n Ä‘á»ƒ trÃ¡nh gÃ¢y hiá»ƒu láº§m cho ngÆ°á»i dÃ¹ng. |
| **Focus Act + CabinetOrder** | Acts lÃ  luáº­t chÃ­nh, Cabinet Orders lÃ  quy Ä‘á»‹nh chi tiáº¿t quan trá»ng. Bá» qua Ministerial Ordinances vÃ  circulars Ä‘á»ƒ giáº£m noise. |

#### 3.4.2. Lá»±a chá»n chiáº¿n lÆ°á»£c Chunking

```mermaid
flowchart TB
    subgraph Options["CÃ¡c phÆ°Æ¡ng Ã¡n Chunking"]
        O1["Fixed-size\n(512 tokens)"]
        O2["Sentence-level"]
        O3["Article-level"]
        O4["Paragraph-level âœ“"]
    end
    
    subgraph Criteria["TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡"]
        C1[Semantic Coherence]
        C2[Context Preservation]
        C3[Citation Precision]
        C4[Embedding Quality]
    end
    
    O4 --> |"Best balance"| C1
    O4 --> |"Article title included"| C2
    O4 --> |"ç¬¬Xæ¡ ç¬¬Yé …"| C3
    O4 --> |"~400 tokens optimal"| C4
```

| PhÆ°Æ¡ng Ã¡n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Káº¿t luáº­n |
|-----------|---------|------------|----------|
| **Fixed-size (512 tokens)** | Äá»“ng Ä‘á»u, tá»‘i Æ°u embedding | CÃ³ thá»ƒ cáº¯t giá»¯a cÃ¢u, máº¥t context | âŒ KhÃ´ng phÃ¹ há»£p phÃ¡p lÃ½ |
| **Sentence-level** | Granular | QuÃ¡ nhá», máº¥t context Ä‘iá»u luáº­t | âŒ KhÃ´ng Ä‘á»§ context |
| **Article-level** | Äáº§y Ä‘á»§ context | QuÃ¡ lá»›n (>2000 tokens), chi phÃ­ cao | âŒ QuÃ¡ lá»›n |
| **Paragraph-level** âœ“ | Semantic coherence, citation chÃ­nh xÃ¡c | Cáº§n thÃªm context tá»« article | âœ… **Lá»±a chá»n** |

**LÃ½ do chá»n Paragraph-level:**
1. **Legal citation standard**: PhÃ¡p luáº­t Nháº­t trÃ­ch dáº«n theo format "ç¬¬Xæ¡ ç¬¬Yé …" (Äiá»u X Khoáº£n Y)
2. **Semantic unit**: Má»—i khoáº£n thÆ°á»ng chá»©a má»™t Ã½ hoÃ n chá»‰nh
3. **Optimal size**: ~400 tokens vá»«a Ä‘á»§ cho embedding quality
4. **Context enrichment**: ThÃªm `law_title + article_title` vÃ o `text_with_context`

#### 3.4.3. Lá»±a chá»n Embedding Model

```mermaid
flowchart LR
    subgraph Models["So sÃ¡nh Embedding Models"]
        M1["OpenAI\ntext-embedding-3-large"]
        M2["OpenAI\ntext-embedding-3-small"]
        M3["Cohere\nembed-multilingual-v3"]
        M4["Local\nmultilingual-e5-large"]
    end
    
    M1 --> |"âœ“ Selected"| R[Reasons]
    R --> R1["3072-dim: cao nháº¥t"]
    R --> R2["Best multilingual"]
    R --> R3["Japanese + Vietnamese"]
    R --> R4["API reliability"]
```

| Model | Dimensions | Multilingual | Cost | Latency | Káº¿t luáº­n |
|-------|------------|--------------|------|---------|----------|
| text-embedding-3-large | 3072 | â­â­â­ | $0.13/1M | 100ms | âœ… **Chá»n** |
| text-embedding-3-small | 1536 | â­â­ | $0.02/1M | 80ms | Chi phÃ­ tháº¥p nhÆ°ng quality kÃ©m hÆ¡n |
| Cohere embed-v3 | 1024 | â­â­â­ | $0.10/1M | 150ms | Tá»‘t nhÆ°ng API Ã­t á»•n Ä‘á»‹nh hÆ¡n |
| multilingual-e5-large | 1024 | â­â­ | Free | 50ms | Local nhÆ°ng cáº§n GPU, quality tháº¥p hÆ¡n |

**LÃ½ do chá»n text-embedding-3-large:**
1. **Highest quality**: 3072 dimensions capture more semantic nuance
2. **Cross-lingual excellence**: Tá»‘t nháº¥t cho Vietnamese query â†’ Japanese document retrieval
3. **Production stability**: OpenAI API reliability cao
4. **Cost-effective**: $0.13/1M tokens há»£p lÃ½ cho 15K chunks

#### 3.4.4. Lá»±a chá»n Vector Database

```mermaid
flowchart TB
    subgraph Comparison["So sÃ¡nh Vector DB"]
        Q[Qdrant Cloud âœ“]
        P[Pinecone]
        W[Weaviate]
        C[ChromaDB]
    end
    
    Q --> F1["Free tier Ä‘á»§ dÃ¹ng"]
    Q --> F2["Native hybrid search"]
    Q --> F3["RRF fusion built-in"]
    Q --> F4["No self-hosting"]
```

| Database | Free Tier | Hybrid Search | Self-hosted | Káº¿t luáº­n |
|----------|-----------|---------------|-------------|----------|
| **Qdrant Cloud** | 1GB free | âœ… Native | âŒ Cloud | âœ… **Chá»n** |
| Pinecone | 100K vectors | âŒ Cáº§n workaround | âŒ Cloud only | KhÃ´ng cÃ³ hybrid search native |
| Weaviate | Self-host free | âœ… Native | âœ… Docker | Cáº§n maintain infrastructure |
| ChromaDB | Local only | âŒ No | âœ… Local | KhÃ´ng cloud, khÃ´ng hybrid |

**LÃ½ do chá»n Qdrant Cloud:**
1. **Free tier**: 1GB Ä‘á»§ cho 15K vectors (3072-dim)
2. **Native hybrid search**: RRF fusion built-in, khÃ´ng cáº§n custom code
3. **Query API**: Single request cho hybrid vá»›i prefetch
4. **No maintenance**: Managed service, khÃ´ng lo server

#### 3.4.5. Lá»±a chá»n Hybrid Search vs Vector-only

```mermaid
flowchart TB
    subgraph Problem["Váº¥n Ä‘á» vá»›i Vector-only"]
        P1["Query: ç¬¬ä¸‰åäºŒæ¡"]
        P1 --> V1["Vector search"]
        V1 --> R1["Miss exact match!\nSemantic khÃ´ng match sá»‘ Ä‘iá»u"]
    end
    
    subgraph Solution["Hybrid Search giáº£i quyáº¿t"]
        P2["Query: ç¬¬ä¸‰åäºŒæ¡"]
        P2 --> H1["Dense: semantic"]
        P2 --> H2["Sparse: BM25 keyword"]
        H1 --> RRF["RRF Fusion"]
        H2 --> RRF
        RRF --> R2["âœ“ Exact match + related"]
    end
```

| Scenario | Vector-only | Hybrid | Winner |
|----------|-------------|--------|--------|
| "Thá»i gian lÃ m viá»‡c" (semantic) | âœ… Good | âœ… Good | Tie |
| "ç¬¬ä¸‰åäºŒæ¡" (exact article) | âŒ Miss | âœ… Match | Hybrid |
| "åŠ´åƒåŸºæº–æ³• æ®‹æ¥­" (mixed) | âš ï¸ OK | âœ… Better | Hybrid |
| Legal terminology (å°‚é–€ç”¨èªž) | âš ï¸ Variable | âœ… BM25 boost | Hybrid |

**LÃ½ do chá»n Hybrid Search:**
1. **Legal domain specificity**: Sá»‘ Ä‘iá»u, tÃªn luáº­t cáº§n exact match
2. **Terminology precision**: BM25 catch thuáº­t ngá»¯ phÃ¡p lÃ½ chÃ­nh xÃ¡c
3. **Best of both worlds**: Semantic understanding + keyword precision
4. **Measurable improvement**: +15-20% retrieval accuracy

#### 3.4.6. Lá»±a chá»n Reranking Strategy

```mermaid
flowchart LR
    subgraph Stage1["Stage 1: Recall"]
        BI["Bi-encoder\n(OpenAI embedding)"]
        BI --> |"Fast O(1)"| T20["Top 20-40 docs"]
    end
    
    subgraph Stage2["Stage 2: Precision"]
        T20 --> CE["Cross-encoder\n(BGE reranker)"]
        CE --> |"Slow O(n)"| T5["Top 5 docs"]
    end
    
    subgraph Why["Táº¡i sao cáº§n Reranking?"]
        W1["Bi-encoder: independent encoding"]
        W2["Cross-encoder: joint encoding"]
        W2 --> |"More accurate"| W3["Better ranking"]
    end
```

| Approach | Accuracy | Latency | Use Case |
|----------|----------|---------|----------|
| Bi-encoder only | â­â­ | 100ms | High throughput, acceptable quality |
| Cross-encoder only | â­â­â­ | 10s+ | Batch processing, highest quality |
| **Two-stage** âœ… | â­â­â­ | 500ms | Best balance accuracy/speed |

**LÃ½ do chá»n Two-stage vá»›i BGE Reranker:**
1. **Quality**: Cross-encoder cÃ³ full attention giá»¯a query-document
2. **Latency trade-off**: Chá»‰ rerank top-20 (khÃ´ng pháº£i 15K docs)
3. **No GPU required**: BGE-reranker-large cháº¡y Ä‘Æ°á»£c trÃªn CPU
4. **Measured improvement**: +10-20% relevance score

#### 3.4.7. Lá»±a chá»n LLM cho Generation

| Model | Quality | Cost | Speed | Context | Káº¿t luáº­n |
|-------|---------|------|-------|---------|----------|
| GPT-4o | â­â­â­ | $15/1M | Slow | 128K | QuÃ¡ Ä‘áº¯t cho production |
| **GPT-4o-mini** âœ… | â­â­â­ | $0.15/1M | Fast | 128K | **Best value** |
| GPT-3.5-turbo | â­â­ | $0.50/1M | Fast | 16K | Quality kÃ©m cho legal |
| Claude 3 Haiku | â­â­ | $0.25/1M | Fast | 200K | Tá»‘t nhÆ°ng khÃ¡c ecosystem |

**LÃ½ do chá»n GPT-4o-mini:**
1. **Cost-effective**: 100x cheaper than GPT-4o, same quality tier
2. **Fast**: Äá»§ nhanh cho interactive chat
3. **Large context**: 128K tokens Ä‘á»§ cho legal documents
4. **Vietnamese fluency**: Tá»‘t cho output tiáº¿ng Viá»‡t vá»›i annotation tiáº¿ng Nháº­t

#### 3.4.8. Lá»±a chá»n Agent Framework

```mermaid
flowchart TB
    subgraph Options["Framework Options"]
        L1[LangChain LCEL]
        L2[LangGraph âœ“]
        L3[Custom Python]
        L4[CrewAI]
    end
    
    L2 --> R1["StateGraph with cycles"]
    L2 --> R2["Conditional routing"]
    L2 --> R3["Self-correction loop"]
    L2 --> R4["Easy debugging"]
```

| Framework | State Management | Cycles/Loops | Debugging | Káº¿t luáº­n |
|-----------|------------------|--------------|-----------|----------|
| LangChain LCEL | âŒ Stateless | âŒ No | âš ï¸ Limited | KhÃ´ng há»— trá»£ self-correction |
| **LangGraph** âœ… | âœ… TypedDict | âœ… Yes | âœ… Easy | **Chá»n** |
| Custom Python | Manual | Manual | Manual | Tá»‘n effort maintain |
| CrewAI | âœ… Built-in | âœ… Yes | âš ï¸ Complex | Overkill cho single agent |

**LÃ½ do chá»n LangGraph:**
1. **Self-correction**: Conditional edges cho phÃ©p retry khi documents khÃ´ng relevant
2. **State management**: TypedDict track query, documents, grades qua cÃ¡c bÆ°á»›c
3. **Modular**: Dá»… thÃªm/sá»­a nodes (translate â†’ retrieve â†’ grade â†’ rerank â†’ generate)
4. **Debugging**: Visualize graph, trace execution path

---

## 4. TRIá»‚N KHAI Há»† THá»NG RAG

### 4.1. Query Processing Pipeline

```mermaid
sequenceDiagram
    participant U as User
    participant API as FastAPI
    participant QT as QueryTranslator
    participant EMB as EmbeddingService
    participant VS as VectorStore
    participant RR as Reranker
    participant LLM as LLM Provider
    
    U->>API: POST /api/chat {"query": "Thuáº¿ thu nháº­p..."}
    API->>QT: translate(query)
    QT->>LLM: Translation prompt
    LLM-->>QT: "æ‰€å¾—ç¨Žã«ã¤ã„ã¦..."
    QT-->>API: translated + keywords + search_queries
    
    loop Multi-Query Retrieval
        API->>EMB: embed_batch(search_queries)
        EMB-->>API: query_vectors[]
        API->>VS: hybrid_search(dense, sparse)
        VS-->>API: results[]
    end
    
    API->>API: Deduplicate & Score Filter
    
    opt Reranking Enabled
        API->>RR: rerank(query, docs, top_k=5)
        RR-->>API: reranked_docs
    end
    
    API->>LLM: generate_with_context(query, context)
    LLM-->>API: answer
    API-->>U: ChatResponse {answer, sources}
```

### 4.2. Query Translation Module

Module `query_translator.py` thá»±c hiá»‡n dá»‹ch vÃ  má»Ÿ rá»™ng query:

```mermaid
flowchart TB
    subgraph Input
        Q[Vietnamese Query:\n"Thá»i gian lÃ m viá»‡c tá»‘i Ä‘a má»—i tuáº§n?"]
    end
    
    subgraph Process["Query Expansion"]
        Q --> LLM[LLM Translation]
        LLM --> TR[translated:\n"é€±ã®æœ€å¤§åŠ´åƒæ™‚é–“ã¯?"]
        LLM --> KW[keywords:\nåŠ´åƒæ™‚é–“, æ³•å®šåŠ´åƒæ™‚é–“, é€±40æ™‚é–“]
        LLM --> RT[related_terms:\nç¬¬ä¸‰åäºŒæ¡, åŠ´åƒåŸºæº–æ³•]
        LLM --> SQ[search_queries:\n- æ³•å®šåŠ´åƒæ™‚é–“ã®ä¸Šé™\n- é€±ã®åŠ´åƒæ™‚é–“åˆ¶é™\n- åŠ´åƒåŸºæº–æ³•ã®åŠ´åƒæ™‚é–“è¦å®š]
    end
    
    subgraph Output["Multi-Query Vector"]
        TR --> MQ[3-5 Search Queries]
        SQ --> MQ
        KW --> MQ
    end
```

**Translation Prompt:**
```python
QUERY_EXPANSION_SYSTEM = """Báº¡n lÃ  chuyÃªn gia phÃ¡p luáº­t lao Ä‘á»™ng Nháº­t Báº£n.
PhÃ¢n tÃ­ch cÃ¢u há»i phÃ¡p lÃ½ vÃ  tráº£ vá» JSON vá»›i:
1. translated: Báº£n dá»‹ch tiáº¿ng Nháº­t
2. keywords: 3-5 keywords phÃ¡p lÃ½ tiáº¿ng Nháº­t
3. related_terms: Sá»‘ Ä‘iá»u, tÃªn luáº­t liÃªn quan
4. search_queries: 2-3 cÃ¢u query tÃ¬m kiáº¿m khÃ¡c nhau

Tráº£ vá» CHÃNH XÃC format JSON:
{"translated": "...", "keywords": [...], "related_terms": [...], "search_queries": [...]}"""
```

### 4.3. Hybrid Search Implementation

```mermaid
flowchart TB
    subgraph Query["Query Processing"]
        Q[Japanese Query] --> DE[Dense Embedding\nOpenAI 3072-dim]
        Q --> SE[Sparse Embedding\nBM25 via fastembed]
    end
    
    subgraph Qdrant["Qdrant Query API"]
        DE --> QA[Query API]
        SE --> QA
        
        QA --> P1[Prefetch: Dense\ntop_k * 4]
        QA --> P2[Prefetch: Sparse\ntop_k * 4]
        
        P1 --> RRF[RRF Fusion]
        P2 --> RRF
    end
    
    subgraph Result["Results"]
        RRF --> F[Final Ranking\ntop_k results]
    end
```

**Implementation Code:**
```python
def hybrid_search(
    self,
    dense_vector: list[float],
    sparse_vector: SparseVector,
    top_k: int = 10,
    filters: dict | None = None,
) -> list[dict]:
    """Hybrid search using Qdrant Query API with RRF fusion."""
    
    prefetch = [
        # Dense vector search
        models.Prefetch(
            query=dense_vector,
            using="dense",
            limit=top_k * 4,
            filter=qdrant_filter,
        ),
        # Sparse vector search  
        models.Prefetch(
            query=sparse_vector,
            using="sparse",
            limit=top_k * 4,
            filter=qdrant_filter,
        ),
    ]
    
    # RRF fusion
    results = self.client.query_points(
        collection_name=self.collection_name,
        prefetch=prefetch,
        query=models.FusionQuery(fusion=models.Fusion.RRF),
        limit=top_k,
    )
```

### 4.4. RAGPipeline Class

Class chÃ­nh orchestrate toÃ n bá»™ RAG flow:

```mermaid
classDiagram
    class RAGPipeline {
        +EmbeddingProvider embedding
        +VectorStore vector_store
        +LLMProvider llm
        +Reranker reranker
        +QueryTranslator translator
        +SparseEmbeddingProvider sparse_embedding
        +HybridVectorStore hybrid_store
        +bool use_hybrid_search
        +int default_top_k
        +float min_score_threshold
        
        +search(query, top_k, filters) list~SearchResult~
        +chat(query, top_k, use_multi_query) ChatResponse
        -_translate_query(query) str
        -_build_context(results) list~str~
        -_to_search_result(result) SearchResult
        -_to_source_document(result) SourceDocument
    }
    
    class EmbeddingProvider {
        <<interface>>
        +embed(text) list~float~
        +embed_batch(texts) list~list~float~~
    }
    
    class VectorStore {
        <<interface>>
        +search(query_vector, top_k, filters) list~dict~
    }
    
    class HybridVectorStore {
        <<interface>>
        +hybrid_search(dense, sparse, top_k) list~dict~
    }
    
    class Reranker {
        <<interface>>
        +rerank(query, docs, top_k) list~dict~
    }
    
    RAGPipeline --> EmbeddingProvider
    RAGPipeline --> VectorStore
    RAGPipeline --> HybridVectorStore
    RAGPipeline --> Reranker
```

**Core Flow trong method `chat()`:**

```mermaid
flowchart TB
    subgraph Step1["1. Query Expansion"]
        Q[User Query] --> MQ[Multi-Query Generation]
        MQ --> Q1[Original Translated]
        MQ --> Q2[Search Query 1]
        MQ --> Q3[Search Query 2]
    end
    
    subgraph Step2["2. Batch Embedding"]
        Q1 --> BE[embed_batch]
        Q2 --> BE
        Q3 --> BE
        BE --> V1[Vector 1]
        BE --> V2[Vector 2]
        BE --> V3[Vector 3]
    end
    
    subgraph Step3["3. Multi-Query Retrieval"]
        V1 --> HS1[Hybrid Search]
        V2 --> HS2[Hybrid Search]
        V3 --> HS3[Hybrid Search]
        HS1 --> DD[Deduplicate\nKeep Highest Score]
        HS2 --> DD
        HS3 --> DD
    end
    
    subgraph Step4["4. Score Filtering"]
        DD --> SF[Filter score >= 0.25]
        SF --> |Fallback| FB[Top 3 if empty]
    end
    
    subgraph Step5["5. Reranking"]
        SF --> RR{Reranker?}
        RR --> |Yes| CR[Cross-Encoder Rerank]
        RR --> |No| TR[Take Top-K]
        CR --> TOP[Top 5 Results]
        TR --> TOP
    end
    
    subgraph Step6["6. Generation"]
        TOP --> BC[Build Context]
        BC --> LLM[LLM Generation]
        LLM --> ANS[Answer + Sources]
    end
```

### 4.5. LangGraph Agent

Agent vá»›i self-correction loop tá»± Ä‘á»™ng cáº£i thiá»‡n retrieval quality:

```mermaid
flowchart TB
    subgraph State["LegalRAGState"]
        S1[query: str]
        S2[translated_query: str]
        S3[search_queries: list]
        S4[documents: list]
        S5[document_grades: list]
        S6[reranked_documents: list]
        S7[answer: str]
        S8[sources: list]
        S9[rewrite_count: int]
    end
    
    subgraph Nodes["Agent Nodes"]
        N1[translate_node]
        N2[retrieve_node]
        N3[grade_documents_node]
        N4[rerank_node]
        N5[generate_node]
        N6[rewrite_query_node]
    end
    
    subgraph Flow["Execution Flow"]
        START --> N1
        N1 --> N2
        N2 --> N3
        N3 --> |relevant >= 2| N4
        N3 --> |relevant < 2\nretry < 2| N6
        N6 --> N2
        N4 --> N5
        N5 --> END
    end
```

**Node Implementations:**

| Node | Function | Input â†’ Output |
|------|----------|----------------|
| `translate_node` | Dá»‹ch + expand query | query â†’ translated_query, search_queries |
| `retrieve_node` | Multi-query vector search | search_queries â†’ documents (top 40) |
| `grade_documents_node` | LLM Ä‘Ã¡nh giÃ¡ relevance | documents â†’ document_grades |
| `rerank_node` | BGE cross-encoder | documents â†’ reranked_documents (top 10) |
| `generate_node` | LLM sinh cÃ¢u tráº£ lá»i | reranked_documents â†’ answer, sources |
| `rewrite_query_node` | Viáº¿t láº¡i query | query â†’ new query + re-translate |

**Self-Correction Logic:**
```python
def should_rewrite(state: LegalRAGState) -> Literal["rerank", "rewrite"]:
    grades = state.get("document_grades", [])
    rewrite_count = state.get("rewrite_count", 0)
    relevant_count = sum(1 for g in grades if g == "relevant")
    
    if relevant_count >= 2 or rewrite_count >= 2:
        return "rerank"
    else:
        return "rewrite"  # Loop back to retrieve
```

### 4.6. Embedding Service

Service tÃ¡i sá»­ dá»¥ng vá»›i error handling vÃ  retry logic:

```mermaid
flowchart TB
    subgraph Input
        T[Texts to Embed]
    end
    
    subgraph Process["EmbeddingService"]
        T --> TR[Truncate\nmax 6000 tokens]
        TR --> BA[Batch API Call]
        
        BA --> |Success| E[Embeddings]
        BA --> |Token Overflow| SP[Split Batch]
        SP --> BA
        
        BA --> |Rate Limit| RT[Retry with Backoff]
        RT --> BA
    end
    
    subgraph Output
        E --> V[3072-dim Vectors]
    end
```

**Key Methods:**
```python
class EmbeddingService:
    def embed_text(self, text: str) -> list[float]:
        """Single text embedding with auto-truncation."""
        
    def embed_batch(self, texts: list[str]) -> list[list[float]]:
        """Batch embedding with overflow handling."""
        
    def embed_batch_numpy(self, texts: list[str]) -> np.ndarray:
        """Return as numpy array for Qdrant."""
```

### 4.7. BGE Reranker Service

Cross-encoder reranking vá»›i lazy loading Ä‘á»ƒ tiáº¿t kiá»‡m memory:

```mermaid
flowchart TB
    subgraph Init["Lazy Initialization"]
        F{First Call?}
        F --> |Yes| L[Load BGE Model\n~2GB RAM]
        F --> |No| C[Use Cached Model]
    end
    
    subgraph Rerank["Reranking Process"]
        Q[Query] --> P[Create Pairs]
        D[Documents] --> P
        P --> |"[query, doc1], [query, doc2], ..."| M[BGE Model]
        M --> S[Compute Scores\nnormalize=True]
        S --> SO[Sort by Score]
        SO --> TK[Return Top-K]
    end
```

**Model Specifications:**
- **Model:** `BAAI/bge-reranker-large`
- **Device:** CPU (no GPU required)
- **Memory:** ~1.5-2GB RAM
- **Latency:** ~500ms for 20 documents

### 4.8. Response Generation

Prompt engineering cho output tiáº¿ng Viá»‡t vá»›i Japanese annotations:

```python
LEGAL_ASSISTANT_SYSTEM = """Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n phÃ¡p luáº­t Nháº­t Báº£n cho ngÆ°á»i Viá»‡t.

Quy táº¯c tráº£ lá»i:
1. Tráº£ lá»i Báº°NG TIáº¾NG VIá»†T
2. Khi Ä‘á» cáº­p thuáº­t ngá»¯ phÃ¡p lÃ½, thÃªm phiÃªn Ã¢m tiáº¿ng Nháº­t trong ngoáº·c
   VÃ­ dá»¥: Luáº­t TiÃªu chuáº©n Lao Ä‘á»™ng (åŠ´åƒåŸºæº–æ³•)
3. LUÃ”N trÃ­ch dáº«n nguá»“n: sá»‘ Ä‘iá»u, tÃªn luáº­t cá»¥ thá»ƒ
4. Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong context, nÃ³i rÃµ "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin"
5. Giáº£i thÃ­ch rÃµ rÃ ng, dá»… hiá»ƒu cho ngÆ°á»i khÃ´ng chuyÃªn phÃ¡p lÃ½

Format cÃ¢u tráº£ lá»i:
- Báº¯t Ä‘áº§u vá»›i cÃ¢u tráº£ lá»i trá»±c tiáº¿p cho cÃ¢u há»i
- Giáº£i thÃ­ch chi tiáº¿t náº¿u cáº§n
- Káº¿t thÃºc vá»›i trÃ­ch dáº«n nguá»“n: ã€Nguá»“n: [TÃªn luáº­t] [Sá»‘ Ä‘iá»u]ã€‘"""
```

### 4.9. Technology Stack

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Backend** | FastAPI | â‰¥0.109.0 | Async REST API |
| **Vector DB** | Qdrant Cloud | Free Tier | Hybrid vector storage |
| **Dense Embedding** | OpenAI text-embedding-3-large | - | 3072-dim embeddings |
| **Sparse Embedding** | fastembed (BM25) | â‰¥0.3.0 | BM25 sparse vectors |
| **LLM** | GPT-4o-mini | - | Generation + Translation |
| **Reranker** | BAAI/bge-reranker-large | - | Cross-encoder |
| **Agent** | LangGraph | â‰¥0.2.0 | Workflow orchestration |
| **XML Processing** | lxml | â‰¥5.0.0 | Legal XML parsing |

---

## 5. THá»¬ NGHIá»†M VÃ€ ÄÃNH GIÃ

### 5.1. Test Dataset

Bá»™ test 20+ cÃ¢u há»i vá» phÃ¡p luáº­t tÃ i chÃ­nh:

| Category | Sample Questions |
|----------|-----------------|
| Thuáº¿ Thu Nháº­p | Thuáº¿ thu nháº­p cÃ¡ nhÃ¢n á»Ÿ Nháº­t tÃ­nh nhÆ° tháº¿ nÃ o? |
| Báº£o Hiá»ƒm XH | Äiá»u kiá»‡n hÆ°á»Ÿng lÆ°Æ¡ng hÆ°u táº¡i Nháº­t? |
| NISA | NISA lÃ  gÃ¬? NgÆ°á»i nÆ°á»›c ngoÃ i cÃ³ thá»ƒ Ä‘Äƒng kÃ½ khÃ´ng? |
| Lao Äá»™ng | Thá»i gian lÃ m viá»‡c tá»‘i Ä‘a má»—i tuáº§n lÃ  bao nhiÃªu giá»? |

### 5.2. RAGAS Evaluation Framework

```mermaid
flowchart TB
    subgraph Metrics["RAGAS Metrics"]
        CP[Context Precision]
        CR[Context Recall]
        F[Faithfulness]
        AR[Answer Relevancy]
    end
    
    subgraph Evaluation
        Q[Query] --> R[Retrieved Context]
        R --> CP
        GT[Ground Truth] --> CR
        R --> CR
        R --> F
        A[Generated Answer] --> F
        A --> AR
        Q --> AR
    end
```

| Metric | Score | Description |
|--------|-------|-------------|
| **Context Precision** | 0.72 | Tá»· lá»‡ context relevant trong retrieved docs |
| **Context Recall** | 0.68 | Coverage of ground truth |
| **Faithfulness** | 0.85 | Answer grounded in context |
| **Answer Relevancy** | 0.78 | Answer addresses query |

### 5.3. Reranker Performance

| Query | Before | After | Improvement |
|-------|--------|-------|-------------|
| Thá»i gian nghá»‰ giá»¯a ca | 0.50 | 0.66 | +32% |
| LÃ m thÃªm giá» gáº¥p Ä‘Ã´i | 0.59 | 0.64 | +8% |
| Sa tháº£i thá»­ viá»‡c | 0.45 | 0.58 | +29% |

**Summary:** 60% queries improved vá»›i average +10-20% score gain.

### 5.4. Latency Optimization

```mermaid
gantt
    title Query Latency Breakdown (Before Optimization)
    dateFormat X
    axisFormat %s
    
    section Original
    Translation      :0, 2s
    Multi-Query (5x) :2s, 8s
    Reranking        :8s, 30s
    Generation       :30s, 35s
```

| Optimization | Before | After | Impact |
|--------------|--------|-------|--------|
| Disable Reranker | 60s | 10s | -83% |
| Hybrid Search | 10s | 8s | -20% |
| Reduce Multi-Query (5â†’3) | 8s | 6s | -25% |
| Batch Embeddings | 6s | 5s | -17% |

---

## 6. Káº¾T LUáº¬N VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N

### 6.1. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

1. **Há»‡ thá»‘ng RAG hoÃ n chá»‰nh** cho phÃ¡p luáº­t tÃ i chÃ­nh Nháº­t Báº£n
2. **Cross-lingual retrieval** Vietnamese â†’ Japanese vá»›i translation & expansion
3. **Hybrid search** káº¿t há»£p semantic (dense) vÃ  keyword (sparse)
4. **Two-stage retrieval** vá»›i bi-encoder + cross-encoder reranking
5. **LangGraph Agent** vá»›i self-correction loop
6. **15,629 chunks** tá»« 233 vÄƒn báº£n luáº­t Ä‘Æ°á»£c index

### 6.2. Háº¡n cháº¿

1. **Latency**: 5-10s response time, chÆ°a real-time
2. **Coverage**: ChÆ°a bao phá»§ háº¿t táº¥t cáº£ categories phÃ¡p luáº­t
3. **Multi-turn**: ChÆ°a cÃ³ conversation memory
4. **Evaluation**: Cáº§n thÃªm human evaluation

### 6.3. HÆ°á»›ng phÃ¡t triá»ƒn

```mermaid
timeline
    title Development Roadmap
    
    section Short-term (3-6 months)
        Conversation Memory : Multi-turn chat support
        Caching Layer       : Redis for faster response
        More Categories     : å›½ç¨Ž, åœ°æ–¹è²¡æ”¿ expansion
        
    section Long-term
        Graph RAG           : Neo4j for law relationships
        Production Deploy   : Cloud hosting + rate limiting
        Mobile App          : iOS/Android native
        Feedback Loop       : User feedback collection
```

### 6.4. BÃ i há»c rÃºt ra

1. **Data quality > Model size**: Chunking strategy áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n retrieval quality
2. **Hybrid search hiá»‡u quáº£**: Káº¿t há»£p dense + sparse cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ recall
3. **Reranker trade-off**: ChÃ­nh xÃ¡c hÆ¡n nhÆ°ng áº£nh hÆ°á»Ÿng latency
4. **Cross-lingual khÃ³**: Query translation cáº§n domain-specific tuning

---

## 7. TÃ€I LIá»†U THAM KHáº¢O

### 7.1. Research Papers

1. Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *NeurIPS 2020*.

2. Karpukhin, V., et al. (2020). "Dense Passage Retrieval for Open-Domain Question Answering." *EMNLP 2020*.

3. Xiao, S., et al. (2023). "BGE: Towards General Text Embeddings with Multi-stage Contrastive Learning." *arXiv*.

4. Wang, L., et al. (2023). "Query2doc: Query Expansion with Large Language Models." *EMNLP 2023*.

### 7.2. Technologies & APIs

5. OpenAI. "Embeddings API Documentation." https://platform.openai.com/docs/guides/embeddings

6. Qdrant. "Vector Database Documentation." https://qdrant.tech/documentation/

7. LangGraph. "Build Agentic Workflows." https://langchain-ai.github.io/langgraph/

8. e-Gov Laws API. "æ³•ä»¤API åˆ©ç”¨ã‚¬ã‚¤ãƒ‰." https://elaws.e-gov.go.jp/apitop/

### 7.3. Frameworks

9. FastAPI. "Modern Python Web Framework." https://fastapi.tiangolo.com/

10. RAGAS. "Evaluation Framework for RAG." https://docs.ragas.io/

---

## PHá»¤ Lá»¤C

### A. HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

```bash
# Clone repository
git clone https://github.com/[username]/norman.git
cd norman

# Backend setup
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with API keys

# Run backend
uvicorn app.main:app --reload --port 8000
```

### B. Environment Variables

```env
# OpenAI
OPENAI_API_KEY=sk-...

# Qdrant Cloud
QDRANT_URL=https://xxx.qdrant.tech
QDRANT_API_KEY=...
QDRANT_COLLECTION_NAME=japanese_laws_hybrid

# RAG Settings
USE_RERANKER=false
USE_HYBRID_SEARCH=true
MULTI_QUERY_COUNT=2
```

### C. API Usage Examples

```bash
# Health check
curl http://localhost:8000/api/health

# Vector search
curl -X POST http://localhost:8000/api/search \
  -H "Content-Type: application/json" \
  -d '{"query": "æ‰€å¾—ç¨Ž", "top_k": 3}'

# Chat (standard RAG)
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "Thuáº¿ thu nháº­p cÃ¡ nhÃ¢n á»Ÿ Nháº­t?", "top_k": 5}'

# Chat (with LangGraph agent)
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "...", "use_agent": true}'
```

---

**Norman - Japanese Financial Law RAG System**  
Version 1.0.0 | January 2026

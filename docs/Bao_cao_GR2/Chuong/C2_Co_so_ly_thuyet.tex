Chương này trình bày các cơ sở lý thuyết làm nền tảng cho hệ thống, bao gồm: (i) Retrieval-Augmented Generation trong phần \ref{sec:rag}, (ii) vector embedding và semantic search trong phần \ref{sec:embedding}, (iii) hybrid search trong phần \ref{sec:hybrid}, (iv) reranking trong phần \ref{sec:rerank}, (v) knowledge graph và GraphRAG trong phần \ref{sec:graph}, và (vi) AI Agent trong phần \ref{sec:agent}.


\section{Retrieval-Augmented Generation (RAG)}
\label{sec:rag}

\subsection{Kiến trúc RAG cơ bản}

Retrieval-Augmented Generation (RAG) là kiến trúc kết hợp giữa hệ thống truy xuất thông tin (Information Retrieval) và mô hình sinh ngôn ngữ (Language Generation). RAG được giới thiệu bởi Lewis et al. (2020) trong bài báo \textit{"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"} tại hội nghị NeurIPS.

\textbf{Ý tưởng cốt lõi}: Thay vì buộc LLM ghi nhớ tất cả kiến thức trong tham số (parametric memory), hệ thống truy xuất tài liệu liên quan từ kho dữ liệu bên ngoài (non-parametric memory) và đưa vào context để LLM sinh câu trả lời.

\textbf{Các thành phần chính}:

\begin{enumerate}
    \item \textbf{Indexing Pipeline} (Offline):
    \begin{itemize}
        \item Thu thập và xử lý dữ liệu nguồn
        \item Chia documents thành các chunks
        \item Tạo embedding vectors và lưu vào vector database
    \end{itemize}
    
    \item \textbf{Retrieval Pipeline} (Runtime):
    \begin{itemize}
        \item Chuyển đổi query thành embedding vector
        \item Tìm kiếm chunks tương tự trong database
        \item Trả về top-k chunks liên quan nhất
    \end{itemize}
    
    \item \textbf{Generation Pipeline} (Runtime):
    \begin{itemize}
        \item Xây dựng prompt từ query và retrieved chunks
        \item LLM sinh câu trả lời dựa trên context
    \end{itemize}
\end{enumerate}

\textbf{Luồng xử lý}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{hinh_luong_xu_ly_rag}
    \caption{Luồng xử lý RAG}
    \label{fig:rag_process}
\end{figure}

\subsection{So sánh RAG với Fine-tuning và Prompt Engineering}

Có ba cách tiếp cận chính để cung cấp domain knowledge cho LLM:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Tiêu chí} & \textbf{Prompt Engineering} & \textbf{Fine-tuning} & \textbf{RAG} \\ \hline
    \textbf{Chi phí} & Thấp & Rất cao & Trung bình \\ \hline
    \textbf{Cập nhật kiến thức} & Manual & Re-train & Chỉ re-index \\ \hline
    \textbf{Trích dẫn nguồn} & Có thể & Không & Có \\ \hline
    \textbf{Hallucination} & Cao & Trung bình & Thấp \\ \hline
    \textbf{Scalability} & Kém (context limit) & Tốt & Tốt \\ \hline
    \textbf{Minh bạch} & Thấp & Rất thấp & Cao \\ \hline
    \end{tabular}
    \caption{So sánh RAG, Fine-tuning và Prompt Engineering}
    \label{tab:rag_comparison}
\end{table}

\textbf{Prompt Engineering}: Cung cấp thông tin trực tiếp trong prompt. Đơn giản nhưng giới hạn bởi context window.

\textbf{Fine-tuning}: Huấn luyện lại model trên dữ liệu domain-specific. Model "nhớ" kiến thức trong parameters nhưng khó cập nhật và tốn kém.

\textbf{RAG}: Truy xuất động từ knowledge base. Dễ cập nhật, có citations, nhưng phức tạp hơn trong triển khai.

\subsection{Các thách thức trong RAG}

\textbf{1. Chunk Quality}: 
\begin{itemize}
    \item Chunking quá nhỏ mất context, quá lớn thì nhiễu
    \item Chunk boundaries có thể cắt giữa câu, mất ngữ nghĩa
\end{itemize}

\textbf{2. Retrieval Quality}:
\begin{itemize}
    \item Vocabulary mismatch: query dùng từ khác với document
    \item Semantic gap: dense embedding có thể miss exact keywords
\end{itemize}

\textbf{3. Generation Quality}:
\begin{itemize}
    \item Lost in the middle: LLM bỏ qua thông tin giữa context dài
    \item Conflicting information từ các chunks khác nhau
\end{itemize}

\textbf{4. Cross-lingual Challenges}:
\begin{itemize}
    \item Query và documents khác ngôn ngữ
    \item Cần translation hoặc cross-lingual embeddings
\end{itemize}

\section{Vector Embedding và Semantic Search}
\label{sec:embedding}

\subsection{Khái niệm Vector Embedding}

\textbf{Định nghĩa}: Vector embedding là quá trình chuyển đổi dữ liệu phi cấu trúc (text, image, audio) thành vector số học trong không gian nhiều chiều. Dữ liệu có ngữ nghĩa tương tự sẽ có vector gần nhau.

\textbf{Lịch sử phát triển}:
\begin{itemize}
    \item \textbf{Word2Vec (2013)}: Mikolov et al., học word embeddings từ co-occurrence
    \item \textbf{GloVe (2014)}: Stanford, kết hợp matrix factorization và local context
    \item \textbf{ELMo (2018)}: Contextualized embeddings từ bidirectional LSTM
    \item \textbf{BERT (2018)}: Transformer-based, pre-trained bidirectional representations
    \item \textbf{Sentence-BERT (2019)}: Optimize BERT cho sentence embeddings
\end{itemize}

\textbf{Các loại Embedding}:

\begin{enumerate}
    \item \textbf{Word-level Embedding}: Mỗi từ có vector cố định, không capture context
    \item \textbf{Contextual Embedding}: Vector khác nhau tùy context (ELMo, BERT)
    \item \textbf{Sentence/Document Embedding}: Một vector cho toàn bộ câu/đoạn
\end{enumerate}

\subsection{Các mô hình Embedding phổ biến}

\textbf{Commercial Models}:
\begin{itemize}
    \item OpenAI text-embedding-3-large/small
    \item Cohere embed-multilingual-v3
    \item Google text-embedding-004
\end{itemize}

\textbf{Open-source Models}:
\begin{itemize}
    \item BGE-M3: Multi-lingual, multi-granularity
    \item multilingual-e5-large: Microsoft, 100+ languages
    \item jina-embeddings-v2: Long context (8K tokens)
\end{itemize}

\textbf{Tiêu chí lựa chọn}:
\begin{itemize}
    \item Dimension size: cao hơn thường capture nhiều thông tin hơn
    \item Multilingual support: quan trọng cho cross-lingual applications
    \item Context length: giới hạn số tokens input
    \item Latency và cost
\end{itemize}

\subsection{Vector Similarity Metrics}

\textbf{1. Cosine Similarity} (phổ biến nhất):

\[ \text{cosine}(A, B) = \frac{A \cdot B}{\|A\| \|B\|} \]

\begin{itemize}
    \item Đo góc giữa hai vectors, không phụ thuộc độ dài
    \item Giá trị từ -1 đến 1
\end{itemize}

\textbf{2. Euclidean Distance (L2)}:

\[ d(A, B) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2} \]

\begin{itemize}
    \item Đo khoảng cách tuyệt đối trong không gian
    \item Nhạy cảm với magnitude
\end{itemize}

\textbf{3. Dot Product}:

\[ \text{dot}(A, B) = \sum_{i=1}^{n} A_i B_i \]

\begin{itemize}
    \item Với normalized vectors, tương đương cosine similarity
    \item Đơn giản và nhanh
\end{itemize}

\textbf{Approximate Nearest Neighbor (ANN)}:

Do exact search O(n) chậm với datasets lớn, ANN algorithms được sử dụng:
\begin{itemize}
    \item \textbf{HNSW}: Graph-based, recall cao
    \item \textbf{IVF}: Cluster-based, memory efficient
    \item \textbf{PQ}: Compression, giảm storage
\end{itemize}

\section{Hybrid Search (Dense + Sparse)}
\label{sec:hybrid}

\subsection{Dense Retrieval vs Sparse Retrieval}

\textbf{Sparse Retrieval - BM25}:

BM25 (Best Matching 25) là thuật toán sparse retrieval dựa trên TF-IDF:

\[ \text{BM25}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k + 1)}{f(q_i, D) + k \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})} \]

Trong đó:
\begin{itemize}
    \item $f(q_i, D)$: Tần suất query term trong document
    \item $|D|$: Độ dài document
    \item $k$, $b$: Hyperparameters (thường k=1.2, b=0.75)
\end{itemize}

\textbf{Đặc điểm}: Exact term matching, nhanh, interpretable, nhưng không hiểu synonyms.

\textbf{Dense Retrieval}:

Sử dụng neural network encode query và documents thành dense vectors, so sánh bằng similarity metrics.

\textbf{Đặc điểm}: Hiểu ngữ nghĩa, synonyms, cross-lingual, nhưng có thể miss exact keywords.

\textbf{So sánh}:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Tiêu chí} & \textbf{Sparse (BM25)} & \textbf{Dense (Embedding)} \\ \hline
    Exact match & Tốt & Có thể miss \\ \hline
    Semantic & Không & Tốt \\ \hline
    Cross-lingual & Không & Có \\ \hline
    Proper nouns & Tốt & Kém \\ \hline
    \end{tabular}
    \caption{So sánh Sparse và Dense Retrieval}
    \label{tab:sparse_dense_compare}
\end{table}

\subsection{Reciprocal Rank Fusion (RRF)}

RRF kết hợp kết quả từ nhiều ranking systems mà không cần normalize scores:

\[ \text{RRF}(d) = \sum_{r \in R} \frac{1}{k + \text{rank}_r(d)} \]

Trong đó:
\begin{itemize}
    \item $\text{rank}_r(d)$: Thứ hạng của document $d$ trong hệ thống $r$
    \item $k$: Hằng số smoothing (thường = 60)
\end{itemize}

\textbf{Ví dụ}:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Document} & \textbf{Dense Rank} & \textbf{Sparse Rank} & \textbf{RRF Score} \\ \hline
    Doc A & 1 & 5 & $1/61 + 1/65 = 0.0318$ \\ \hline
    Doc B & 3 & 1 & $1/63 + 1/61 = 0.0323$ \\ \hline
    \end{tabular}
    \caption{Ví dụ tính điểm RRF}
    \label{tab:rrf_example}
\end{table}

\textbf{Ưu điểm}: Không cần normalize, robust với outliers, đơn giản.

\subsection{Ưu điểm của Hybrid Search}

\begin{enumerate}
    \item \textbf{Complementary Strengths}: Dense bắt semantic, sparse bắt exact matches
    \item \textbf{Robustness}: Backup nếu một hệ thống fail
    \item \textbf{Long-tail Queries}: Rare terms có thể không có tốt embeddings, BM25 vẫn match được
\end{enumerate}

\section{Reranking (Two-Stage Retrieval)}
\label{sec:rerank}

\subsection{Bi-Encoder vs Cross-Encoder}

\textbf{Bi-Encoder}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{hinh_bi_encoder}
    \caption{Mô hình Bi-Encoder}
    \label{fig:bi_encoder}
\end{figure}

\begin{itemize}
    \item Encode độc lập, vectors có thể pre-compute
    \item Nhanh: O(log n) với ANN
\end{itemize}

\textbf{Cross-Encoder}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{hinh_cross_encoder}
    \caption{Mô hình Cross-Encoder}
    \label{fig:cross_encoder}
\end{figure}

\begin{itemize}
    \item Encode cùng nhau, full cross-attention
    \item Chậm: O(n), không thể pre-index
    \item Accurate hơn
\end{itemize}

\textbf{So sánh}:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Tiêu chí} & \textbf{Bi-Encoder} & \textbf{Cross-Encoder} \\ \hline
    Speed & ~1ms/doc & ~50-100ms/doc \\ \hline
    Accuracy & Tốt & Rất tốt \\ \hline
    Pre-indexing & Có & Không \\ \hline
    Use case & First-stage & Second-stage \\ \hline
    \end{tabular}
    \caption{So sánh Bi-Encoder và Cross-Encoder}
    \label{tab:encoder_compare}
\end{table}

\textbf{Two-Stage Pipeline}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{hinh_two_stage_candidate}
    \caption{Quy trình Two-Stage Retrieval}
    \label{fig:two_stage}
\end{figure}

\subsection{Các mô hình Reranker phổ biến}

\textbf{Commercial}: Cohere Rerank, Jina Reranker

\textbf{Open-source}:
\begin{itemize}
    \item BGE-Reranker: High quality, multilingual
    \item ms-marco-MiniLM: Fast, English-focused
    \item mmarco-mMiniLM: Multilingual, balanced
\end{itemize}

\textbf{Trade-offs}: Larger models = higher accuracy, higher latency, higher resource usage.

\section{Knowledge Graph và Graph RAG}
\label{sec:graph}

\subsection{Khái niệm Knowledge Graph}

\textbf{Định nghĩa}: Knowledge Graph là cấu trúc dữ liệu biểu diễn thông tin dưới dạng đồ thị với nodes (entities) và edges (relationships).

\textbf{Thành phần}:

\begin{enumerate}
    \item \textbf{Nodes (Entities)}: Objects, concepts với type và properties
    \item \textbf{Edges (Relationships)}: Connections có type giữa nodes
    \item \textbf{Properties}: Attributes key-value
\end{enumerate}

\textbf{Triple Pattern}:
\begin{verbatim}
(Subject) -[Predicate]-> (Object)
(Law) -[HAS_ARTICLE]-> (Article)
(Article) -[REFERENCES]-> (Article)
\end{verbatim}

\subsection{Graph Database và Neo4j}

\textbf{Graph Database}: Hệ thống tối ưu cho lưu trữ và query graph data. Native store nodes và edges, không cần JOIN như relational DB.

\textbf{Ưu điểm}:
\begin{itemize}
    \item Traversal O(1) per hop
    \item Flexible schema
    \item Intuitive modeling
\end{itemize}

\textbf{Cypher Query Language} (Neo4j):
\begin{verbatim}
-- Tìm articles thuộc một law
MATCH (l:Law)-[:HAS_ARTICLE]->(a:Article)
WHERE l.law_id = "law_5"
RETURN a

-- Multi-hop traversal
MATCH (a1:Article)-[:REFERENCES*1..2]->(a2:Article)
RETURN a2
\end{verbatim}

\subsection{Graph RAG: Kết hợp Knowledge Graph với RAG}

\textbf{Motivation}: Vector RAG tốt cho semantic similarity nhưng kém cho:
\begin{itemize}
    \item Entity-specific queries: "Điều 15 nói gì?"
    \item Multi-hop reasoning: "Điều nào reference đến Điều 15?"
    \item Structural queries: "Luật X có bao nhiêu chương?"
\end{itemize}

\textbf{GraphRAG Architecture}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{hinh_graphrag_architecture}
    \caption{Kiến trúc GraphRAG}
    \label{fig:graphrag_arch}
\end{figure}

\textbf{Graph Search Methods}:
\begin{enumerate}
    \item \textbf{Entity Lookup}: Tìm chính xác entity
    \item \textbf{Relation Traversal}: Đi theo edges
    \item \textbf{Aggregation}: Count, sum operations
\end{enumerate}

\subsection{So sánh Vector RAG vs Graph RAG}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Query Type} & \textbf{Vector RAG} & \textbf{Graph RAG} \\ \hline
    Semantic similarity & Xuất sắc & Không \\ \hline
    Entity lookup & Approximate & Exact \\ \hline
    Multi-hop & Không & Native \\ \hline
    Cross-lingual & Có & Không \\ \hline
    Aggregation & Không & Native \\ \hline
    \end{tabular}
    \caption{So sánh Vector RAG và Graph RAG}
    \label{tab:rag_compare}
\end{table}

\section{AI Agent với LangGraph}
\label{sec:agent}

\subsection{Agent và State Machine}

\textbf{AI Agent}: Hệ thống AI có khả năng:
\begin{itemize}
    \item Perceive (nhận input)
    \item Reason (lập kế hoạch)
    \item Act (gọi tools)
    \item Adapt (học từ feedback)
\end{itemize}

\textbf{Agent trong RAG} không chỉ linear pipeline, mà có thể:
\begin{itemize}
    \item Quyết định retrieve thêm nếu chưa đủ
    \item Rewrite query nếu results không relevant
    \item Sử dụng nhiều tools (vector, graph, web search)
\end{itemize}

\textbf{State Machine Approach}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{hinh_state_machine_agent}
    \caption{Sơ đồ State Machine cho RAG}
    \label{fig:state_machine}
\end{figure}

\textbf{LangGraph}: Framework build stateful applications:
\begin{itemize}
    \item \textbf{Nodes}: Processing steps
    \item \textbf{Edges}: Transitions
    \item \textbf{State}: Shared data
    \item \textbf{Conditional Edges}: Dynamic routing
\end{itemize}

\subsection{Self-Correction trong RAG}

\textbf{1. Document Grading}:
\begin{itemize}
    \item LLM đánh giá mỗi document: relevant hay không?
    \item Filter irrelevant documents
    \item Nếu không đủ relevant docs $\rightarrow$ trigger rewrite
\end{itemize}

\textbf{2. Query Rewriting}:
\begin{itemize}
    \item Nếu retrieval kém, LLM rewrite query
    \item Thử alternative phrasings, keywords
    \item Retry retrieval
\end{itemize}

\textbf{3. Hallucination Check}:
\begin{itemize}
    \item Verify claims against source documents
    \item Regenerate hoặc add disclaimer nếu phát hiện unsupported claims
\end{itemize}

\chapter*{Kết chương}
\addcontentsline{toc}{chapter}{Kết chương}

Chương này đã trình bày các cơ sở lý thuyết quan trọng:

\begin{enumerate}
    \item \textbf{RAG}: Kiến trúc kết hợp retrieval và generation, ưu việt về khả năng cập nhật và trích dẫn nguồn.
    
    \item \textbf{Vector Embedding}: Chuyển đổi text thành vectors, cho phép semantic search dựa trên similarity metrics.
    
    \item \textbf{Hybrid Search}: Kết hợp dense (semantic) và sparse (BM25) bằng RRF fusion để tận dụng ưu điểm cả hai.
    
    \item \textbf{Reranking}: Two-stage retrieval với cross-encoder để cải thiện precision.
    
    \item \textbf{Knowledge Graph}: Cấu trúc graph cho entity lookup và multi-hop reasoning, GraphRAG kết hợp với vector RAG.
    
    \item \textbf{AI Agent}: State machine approach với self-correction để tăng robustness của hệ thống.
\end{enumerate}

Chương tiếp theo sẽ trình bày phân tích yêu cầu và thiết kế hệ thống dựa trên các lý thuyết này.